{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      "Product                         60000 non-null object\n",
      "Consumer complaint narrative    60000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv(\"Bank_complaints.csv\")\n",
    "print(df.info())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000) #Initialize a tokenizer.\n",
    "\n",
    "tokenizer.fit_on_texts(complaints) #Fit it to the complaints\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "np.shape(one_hot_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)  \n",
    "product_onehot = to_categorical(product_cat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, \n",
    "                                                    product_onehot, \n",
    "                                                    test_size=1500, \n",
    "                                                    random_state =42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #1st hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))#2nd hidden layer\n",
    "model.add(layers.Dense(7, activation='softmax'))#output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "8500/8500 [==============================] - 0s 34us/step - loss: 1.9488 - acc: 0.1411 - val_loss: 1.9352 - val_acc: 0.1770\n",
      "Epoch 2/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.9301 - acc: 0.1807 - val_loss: 1.9185 - val_acc: 0.2080\n",
      "Epoch 3/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.9125 - acc: 0.2126 - val_loss: 1.9019 - val_acc: 0.2370\n",
      "Epoch 4/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.8938 - acc: 0.2388 - val_loss: 1.8829 - val_acc: 0.2550\n",
      "Epoch 5/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.8735 - acc: 0.2621 - val_loss: 1.8609 - val_acc: 0.2820\n",
      "Epoch 6/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.8504 - acc: 0.2840 - val_loss: 1.8347 - val_acc: 0.2920\n",
      "Epoch 7/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.8230 - acc: 0.3109 - val_loss: 1.8027 - val_acc: 0.3320\n",
      "Epoch 8/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.7908 - acc: 0.3385 - val_loss: 1.7653 - val_acc: 0.3630\n",
      "Epoch 9/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.7537 - acc: 0.3684 - val_loss: 1.7225 - val_acc: 0.4000\n",
      "Epoch 10/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.7110 - acc: 0.4086 - val_loss: 1.6730 - val_acc: 0.4350\n",
      "Epoch 11/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 1.6638 - acc: 0.4378 - val_loss: 1.6218 - val_acc: 0.4810\n",
      "Epoch 12/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.6118 - acc: 0.4752 - val_loss: 1.5658 - val_acc: 0.5240\n",
      "Epoch 13/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.5561 - acc: 0.5086 - val_loss: 1.5058 - val_acc: 0.5490\n",
      "Epoch 14/120\n",
      "8500/8500 [==============================] - 0s 17us/step - loss: 1.4968 - acc: 0.5355 - val_loss: 1.4427 - val_acc: 0.5710\n",
      "Epoch 15/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.4367 - acc: 0.5611 - val_loss: 1.3809 - val_acc: 0.6090\n",
      "Epoch 16/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.3767 - acc: 0.5844 - val_loss: 1.3194 - val_acc: 0.6210\n",
      "Epoch 17/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.3181 - acc: 0.6049 - val_loss: 1.2627 - val_acc: 0.6370\n",
      "Epoch 18/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.2608 - acc: 0.6198 - val_loss: 1.2066 - val_acc: 0.6460\n",
      "Epoch 19/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.2064 - acc: 0.6347 - val_loss: 1.1504 - val_acc: 0.6590\n",
      "Epoch 20/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.1542 - acc: 0.6448 - val_loss: 1.1025 - val_acc: 0.6700\n",
      "Epoch 21/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.1058 - acc: 0.6576 - val_loss: 1.0535 - val_acc: 0.6800\n",
      "Epoch 22/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.0621 - acc: 0.6688 - val_loss: 1.0083 - val_acc: 0.6910\n",
      "Epoch 23/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 1.0221 - acc: 0.6775 - val_loss: 0.9685 - val_acc: 0.7010\n",
      "Epoch 24/120\n",
      "8500/8500 [==============================] - 0s 17us/step - loss: 0.9858 - acc: 0.6840 - val_loss: 0.9344 - val_acc: 0.7100\n",
      "Epoch 25/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.9533 - acc: 0.6951 - val_loss: 0.9037 - val_acc: 0.7050\n",
      "Epoch 26/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.9234 - acc: 0.7020 - val_loss: 0.8772 - val_acc: 0.7090\n",
      "Epoch 27/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.8971 - acc: 0.7064 - val_loss: 0.8525 - val_acc: 0.7200\n",
      "Epoch 28/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.8723 - acc: 0.7125 - val_loss: 0.8260 - val_acc: 0.7320\n",
      "Epoch 29/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.8502 - acc: 0.7173 - val_loss: 0.8082 - val_acc: 0.7320\n",
      "Epoch 30/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.8297 - acc: 0.7238 - val_loss: 0.7855 - val_acc: 0.7400\n",
      "Epoch 31/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.8105 - acc: 0.7271 - val_loss: 0.7653 - val_acc: 0.7410\n",
      "Epoch 32/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.7933 - acc: 0.7332 - val_loss: 0.7495 - val_acc: 0.7480\n",
      "Epoch 33/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.7776 - acc: 0.7351 - val_loss: 0.7364 - val_acc: 0.7460\n",
      "Epoch 34/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.7630 - acc: 0.7401 - val_loss: 0.7185 - val_acc: 0.7560\n",
      "Epoch 35/120\n",
      "8500/8500 [==============================] - 0s 17us/step - loss: 0.7492 - acc: 0.7446 - val_loss: 0.7035 - val_acc: 0.7510\n",
      "Epoch 36/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.7365 - acc: 0.7459 - val_loss: 0.6926 - val_acc: 0.7580\n",
      "Epoch 37/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.7243 - acc: 0.7528 - val_loss: 0.6812 - val_acc: 0.7620\n",
      "Epoch 38/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.7129 - acc: 0.7515 - val_loss: 0.6732 - val_acc: 0.7580\n",
      "Epoch 39/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.7024 - acc: 0.7561 - val_loss: 0.6588 - val_acc: 0.7640\n",
      "Epoch 40/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6921 - acc: 0.7611 - val_loss: 0.6524 - val_acc: 0.7670\n",
      "Epoch 41/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6829 - acc: 0.7614 - val_loss: 0.6414 - val_acc: 0.7710\n",
      "Epoch 42/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6739 - acc: 0.7667 - val_loss: 0.6282 - val_acc: 0.7740\n",
      "Epoch 43/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6648 - acc: 0.7655 - val_loss: 0.6239 - val_acc: 0.7720\n",
      "Epoch 44/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6565 - acc: 0.7695 - val_loss: 0.6159 - val_acc: 0.7790\n",
      "Epoch 45/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6489 - acc: 0.7726 - val_loss: 0.6066 - val_acc: 0.7820\n",
      "Epoch 46/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6412 - acc: 0.7741 - val_loss: 0.6053 - val_acc: 0.7790\n",
      "Epoch 47/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.6346 - acc: 0.7778 - val_loss: 0.5931 - val_acc: 0.7850\n",
      "Epoch 48/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6273 - acc: 0.7791 - val_loss: 0.5868 - val_acc: 0.7800\n",
      "Epoch 49/120\n",
      "8500/8500 [==============================] - 0s 17us/step - loss: 0.6204 - acc: 0.7808 - val_loss: 0.5769 - val_acc: 0.7860\n",
      "Epoch 50/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6138 - acc: 0.7847 - val_loss: 0.5716 - val_acc: 0.7870\n",
      "Epoch 51/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6077 - acc: 0.7848 - val_loss: 0.5691 - val_acc: 0.7930\n",
      "Epoch 52/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.6018 - acc: 0.7860 - val_loss: 0.5652 - val_acc: 0.7930\n",
      "Epoch 53/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.5957 - acc: 0.7894 - val_loss: 0.5527 - val_acc: 0.7950\n",
      "Epoch 54/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5898 - acc: 0.7936 - val_loss: 0.5498 - val_acc: 0.8000\n",
      "Epoch 55/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.5846 - acc: 0.7952 - val_loss: 0.5426 - val_acc: 0.7970\n",
      "Epoch 56/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.5788 - acc: 0.7939 - val_loss: 0.5388 - val_acc: 0.8090\n",
      "Epoch 57/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5742 - acc: 0.7985 - val_loss: 0.5365 - val_acc: 0.8000\n",
      "Epoch 58/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5691 - acc: 0.7981 - val_loss: 0.5335 - val_acc: 0.8040\n",
      "Epoch 59/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5637 - acc: 0.7996 - val_loss: 0.5245 - val_acc: 0.8070\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5582 - acc: 0.8025 - val_loss: 0.5292 - val_acc: 0.8040\n",
      "Epoch 61/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5543 - acc: 0.8045 - val_loss: 0.5150 - val_acc: 0.8120\n",
      "Epoch 62/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5495 - acc: 0.8054 - val_loss: 0.5171 - val_acc: 0.8070\n",
      "Epoch 63/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5451 - acc: 0.8062 - val_loss: 0.5074 - val_acc: 0.8120\n",
      "Epoch 64/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5399 - acc: 0.8121 - val_loss: 0.5049 - val_acc: 0.8100\n",
      "Epoch 65/120\n",
      "8500/8500 [==============================] - ETA: 0s - loss: 0.5358 - acc: 0.811 - 0s 15us/step - loss: 0.5361 - acc: 0.8108 - val_loss: 0.4998 - val_acc: 0.8130\n",
      "Epoch 66/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5316 - acc: 0.8120 - val_loss: 0.4934 - val_acc: 0.8210\n",
      "Epoch 67/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5272 - acc: 0.8152 - val_loss: 0.4902 - val_acc: 0.8190\n",
      "Epoch 68/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5233 - acc: 0.8172 - val_loss: 0.4851 - val_acc: 0.8200\n",
      "Epoch 69/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5192 - acc: 0.8198 - val_loss: 0.4831 - val_acc: 0.8220\n",
      "Epoch 70/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5150 - acc: 0.8209 - val_loss: 0.4814 - val_acc: 0.8230\n",
      "Epoch 71/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5115 - acc: 0.8220 - val_loss: 0.4754 - val_acc: 0.8220\n",
      "Epoch 72/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.5070 - acc: 0.8255 - val_loss: 0.4720 - val_acc: 0.8230\n",
      "Epoch 73/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.5036 - acc: 0.8234 - val_loss: 0.4687 - val_acc: 0.8330\n",
      "Epoch 74/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4995 - acc: 0.8258 - val_loss: 0.4651 - val_acc: 0.8390\n",
      "Epoch 75/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4963 - acc: 0.8259 - val_loss: 0.4634 - val_acc: 0.8300\n",
      "Epoch 76/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4926 - acc: 0.8282 - val_loss: 0.4559 - val_acc: 0.8340\n",
      "Epoch 77/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4886 - acc: 0.8324 - val_loss: 0.4526 - val_acc: 0.8390\n",
      "Epoch 78/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4850 - acc: 0.8327 - val_loss: 0.4491 - val_acc: 0.8420\n",
      "Epoch 79/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4818 - acc: 0.8333 - val_loss: 0.4481 - val_acc: 0.8460\n",
      "Epoch 80/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4784 - acc: 0.8356 - val_loss: 0.4471 - val_acc: 0.8380\n",
      "Epoch 81/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4752 - acc: 0.8362 - val_loss: 0.4433 - val_acc: 0.8400\n",
      "Epoch 82/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4712 - acc: 0.8386 - val_loss: 0.4464 - val_acc: 0.8280\n",
      "Epoch 83/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4686 - acc: 0.8369 - val_loss: 0.4353 - val_acc: 0.8470\n",
      "Epoch 84/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4651 - acc: 0.8398 - val_loss: 0.4327 - val_acc: 0.8550\n",
      "Epoch 85/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4615 - acc: 0.8407 - val_loss: 0.4344 - val_acc: 0.8520\n",
      "Epoch 86/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4584 - acc: 0.8425 - val_loss: 0.4308 - val_acc: 0.8460\n",
      "Epoch 87/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4556 - acc: 0.8445 - val_loss: 0.4238 - val_acc: 0.8600\n",
      "Epoch 88/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4522 - acc: 0.8468 - val_loss: 0.4207 - val_acc: 0.8510\n",
      "Epoch 89/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.4489 - acc: 0.8475 - val_loss: 0.4164 - val_acc: 0.8620\n",
      "Epoch 90/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.4463 - acc: 0.8485 - val_loss: 0.4159 - val_acc: 0.8630\n",
      "Epoch 91/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4428 - acc: 0.8486 - val_loss: 0.4104 - val_acc: 0.8600\n",
      "Epoch 92/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4396 - acc: 0.8496 - val_loss: 0.4107 - val_acc: 0.8680\n",
      "Epoch 93/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4371 - acc: 0.8527 - val_loss: 0.4076 - val_acc: 0.8630\n",
      "Epoch 94/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4336 - acc: 0.8529 - val_loss: 0.4032 - val_acc: 0.8620\n",
      "Epoch 95/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4312 - acc: 0.8535 - val_loss: 0.4006 - val_acc: 0.8660\n",
      "Epoch 96/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4283 - acc: 0.8568 - val_loss: 0.3975 - val_acc: 0.8710\n",
      "Epoch 97/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4255 - acc: 0.8559 - val_loss: 0.3976 - val_acc: 0.8670\n",
      "Epoch 98/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4223 - acc: 0.8581 - val_loss: 0.3925 - val_acc: 0.8650\n",
      "Epoch 99/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4202 - acc: 0.8587 - val_loss: 0.3882 - val_acc: 0.8710\n",
      "Epoch 100/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4170 - acc: 0.8598 - val_loss: 0.3880 - val_acc: 0.8710\n",
      "Epoch 101/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4144 - acc: 0.8622 - val_loss: 0.3858 - val_acc: 0.8750\n",
      "Epoch 102/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4111 - acc: 0.8624 - val_loss: 0.3820 - val_acc: 0.8780\n",
      "Epoch 103/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4086 - acc: 0.8641 - val_loss: 0.3809 - val_acc: 0.8780\n",
      "Epoch 104/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4060 - acc: 0.8658 - val_loss: 0.3786 - val_acc: 0.8750\n",
      "Epoch 105/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4033 - acc: 0.8674 - val_loss: 0.3831 - val_acc: 0.8710\n",
      "Epoch 106/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.4010 - acc: 0.8664 - val_loss: 0.3752 - val_acc: 0.8770\n",
      "Epoch 107/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3978 - acc: 0.8684 - val_loss: 0.3728 - val_acc: 0.8790\n",
      "Epoch 108/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3952 - acc: 0.8700 - val_loss: 0.3687 - val_acc: 0.8860\n",
      "Epoch 109/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3929 - acc: 0.8696 - val_loss: 0.3662 - val_acc: 0.8770\n",
      "Epoch 110/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3904 - acc: 0.8714 - val_loss: 0.3639 - val_acc: 0.8810\n",
      "Epoch 111/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.3876 - acc: 0.8733 - val_loss: 0.3613 - val_acc: 0.8870\n",
      "Epoch 112/120\n",
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.3852 - acc: 0.8734 - val_loss: 0.3617 - val_acc: 0.8810\n",
      "Epoch 113/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3826 - acc: 0.8733 - val_loss: 0.3589 - val_acc: 0.8820\n",
      "Epoch 114/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3805 - acc: 0.8745 - val_loss: 0.3567 - val_acc: 0.8820\n",
      "Epoch 115/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3778 - acc: 0.8766 - val_loss: 0.3572 - val_acc: 0.8810\n",
      "Epoch 116/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3758 - acc: 0.8764 - val_loss: 0.3568 - val_acc: 0.8800\n",
      "Epoch 117/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3729 - acc: 0.8761 - val_loss: 0.3484 - val_acc: 0.8860\n",
      "Epoch 118/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3706 - acc: 0.8785 - val_loss: 0.3518 - val_acc: 0.8830\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500/8500 [==============================] - 0s 16us/step - loss: 0.3684 - acc: 0.8793 - val_loss: 0.3492 - val_acc: 0.8840\n",
      "Epoch 120/120\n",
      "8500/8500 [==============================] - 0s 15us/step - loss: 0.3659 - acc: 0.8800 - val_loss: 0.3419 - val_acc: 0.8880\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 27us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36714998111724856, 0.8796]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6266348145802816, 0.768]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9313 - acc: 0.1723 - val_loss: 1.9203 - val_acc: 0.1930\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9053 - acc: 0.2023 - val_loss: 1.8995 - val_acc: 0.2090\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8837 - acc: 0.2231 - val_loss: 1.8758 - val_acc: 0.2160\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8608 - acc: 0.2375 - val_loss: 1.8507 - val_acc: 0.2350\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8356 - acc: 0.2511 - val_loss: 1.8242 - val_acc: 0.2570\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8081 - acc: 0.2748 - val_loss: 1.7949 - val_acc: 0.2820\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7779 - acc: 0.3039 - val_loss: 1.7632 - val_acc: 0.3070\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7438 - acc: 0.3343 - val_loss: 1.7275 - val_acc: 0.3620\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7038 - acc: 0.3851 - val_loss: 1.6833 - val_acc: 0.4040\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6542 - acc: 0.4392 - val_loss: 1.6284 - val_acc: 0.4480\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5998 - acc: 0.4769 - val_loss: 1.5752 - val_acc: 0.4910\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5445 - acc: 0.5111 - val_loss: 1.5186 - val_acc: 0.5170\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4880 - acc: 0.5421 - val_loss: 1.4629 - val_acc: 0.5510\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4315 - acc: 0.5697 - val_loss: 1.4065 - val_acc: 0.5720\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3756 - acc: 0.5891 - val_loss: 1.3513 - val_acc: 0.5990\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3209 - acc: 0.6085 - val_loss: 1.2957 - val_acc: 0.6170\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2681 - acc: 0.6281 - val_loss: 1.2454 - val_acc: 0.6280\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2178 - acc: 0.6387 - val_loss: 1.1946 - val_acc: 0.6370\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1703 - acc: 0.6512 - val_loss: 1.1514 - val_acc: 0.6550\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1255 - acc: 0.6604 - val_loss: 1.1089 - val_acc: 0.6750\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0838 - acc: 0.6727 - val_loss: 1.0677 - val_acc: 0.6770\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0452 - acc: 0.6779 - val_loss: 1.0319 - val_acc: 0.6950\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0100 - acc: 0.6859 - val_loss: 0.9976 - val_acc: 0.6940\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9773 - acc: 0.6931 - val_loss: 0.9671 - val_acc: 0.6980\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9477 - acc: 0.6985 - val_loss: 0.9420 - val_acc: 0.6960\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9205 - acc: 0.7064 - val_loss: 0.9183 - val_acc: 0.7090\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8951 - acc: 0.7115 - val_loss: 0.8981 - val_acc: 0.7060\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8722 - acc: 0.7155 - val_loss: 0.8726 - val_acc: 0.7140\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8506 - acc: 0.7219 - val_loss: 0.8612 - val_acc: 0.7140\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8314 - acc: 0.7240 - val_loss: 0.8401 - val_acc: 0.7240\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8130 - acc: 0.7292 - val_loss: 0.8210 - val_acc: 0.7210\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7960 - acc: 0.7352 - val_loss: 0.8103 - val_acc: 0.7280\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7803 - acc: 0.7387 - val_loss: 0.7960 - val_acc: 0.7320\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7656 - acc: 0.7413 - val_loss: 0.7843 - val_acc: 0.7290\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7519 - acc: 0.7484 - val_loss: 0.7737 - val_acc: 0.7370\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7391 - acc: 0.7459 - val_loss: 0.7678 - val_acc: 0.7330\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7273 - acc: 0.7487 - val_loss: 0.7522 - val_acc: 0.7340\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7161 - acc: 0.7540 - val_loss: 0.7435 - val_acc: 0.7350\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7050 - acc: 0.7580 - val_loss: 0.7405 - val_acc: 0.7350\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6948 - acc: 0.7604 - val_loss: 0.7299 - val_acc: 0.7450\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6855 - acc: 0.7633 - val_loss: 0.7266 - val_acc: 0.7390\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6765 - acc: 0.7651 - val_loss: 0.7168 - val_acc: 0.7430\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6675 - acc: 0.7696 - val_loss: 0.7089 - val_acc: 0.7460\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6594 - acc: 0.7717 - val_loss: 0.7066 - val_acc: 0.7490\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6516 - acc: 0.7745 - val_loss: 0.7041 - val_acc: 0.7460\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6438 - acc: 0.7769 - val_loss: 0.6940 - val_acc: 0.7500\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6365 - acc: 0.7805 - val_loss: 0.6886 - val_acc: 0.7460\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6298 - acc: 0.7808 - val_loss: 0.6836 - val_acc: 0.7460\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6235 - acc: 0.7820 - val_loss: 0.6834 - val_acc: 0.7490\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6168 - acc: 0.7864 - val_loss: 0.6830 - val_acc: 0.7520\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6104 - acc: 0.7879 - val_loss: 0.6773 - val_acc: 0.7530\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.6045 - acc: 0.7900 - val_loss: 0.6702 - val_acc: 0.7560\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5982 - acc: 0.7893 - val_loss: 0.6676 - val_acc: 0.7550\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5926 - acc: 0.7927 - val_loss: 0.6640 - val_acc: 0.7510\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5874 - acc: 0.7937 - val_loss: 0.6625 - val_acc: 0.7550\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5818 - acc: 0.7973 - val_loss: 0.6598 - val_acc: 0.7600\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5766 - acc: 0.7992 - val_loss: 0.6551 - val_acc: 0.7630\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5712 - acc: 0.7997 - val_loss: 0.6606 - val_acc: 0.7510\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5670 - acc: 0.8000 - val_loss: 0.6503 - val_acc: 0.7610\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5617 - acc: 0.8025 - val_loss: 0.6489 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 28us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5573738614082336, 0.8044]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.671012680053711, 0.752]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.5887 - acc: 0.1775 - val_loss: 2.5728 - val_acc: 0.1900\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.5627 - acc: 0.2063 - val_loss: 2.5482 - val_acc: 0.2110\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.5373 - acc: 0.2203 - val_loss: 2.5243 - val_acc: 0.2260\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.5128 - acc: 0.2291 - val_loss: 2.5009 - val_acc: 0.2330\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.4881 - acc: 0.2401 - val_loss: 2.4767 - val_acc: 0.2500\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.4622 - acc: 0.2572 - val_loss: 2.4510 - val_acc: 0.2610\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4340 - acc: 0.2763 - val_loss: 2.4221 - val_acc: 0.2730\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4028 - acc: 0.2959 - val_loss: 2.3894 - val_acc: 0.2830\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.3686 - acc: 0.3183 - val_loss: 2.3537 - val_acc: 0.3120\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.3312 - acc: 0.3495 - val_loss: 2.3154 - val_acc: 0.3380\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.2910 - acc: 0.3787 - val_loss: 2.2735 - val_acc: 0.3790\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.2476 - acc: 0.4156 - val_loss: 2.2293 - val_acc: 0.4010\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.2016 - acc: 0.4484 - val_loss: 2.1837 - val_acc: 0.4600\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1542 - acc: 0.4871 - val_loss: 2.1360 - val_acc: 0.4890\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1049 - acc: 0.5131 - val_loss: 2.0886 - val_acc: 0.5300\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0553 - acc: 0.5421 - val_loss: 2.0383 - val_acc: 0.5420\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0046 - acc: 0.5692 - val_loss: 1.9904 - val_acc: 0.5690\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9555 - acc: 0.5887 - val_loss: 1.9417 - val_acc: 0.5980\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9071 - acc: 0.6081 - val_loss: 1.8951 - val_acc: 0.6110\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8608 - acc: 0.6239 - val_loss: 1.8491 - val_acc: 0.6210\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8161 - acc: 0.6368 - val_loss: 1.8062 - val_acc: 0.6370\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7740 - acc: 0.6497 - val_loss: 1.7661 - val_acc: 0.6480\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7340 - acc: 0.6623 - val_loss: 1.7279 - val_acc: 0.6600\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6960 - acc: 0.6711 - val_loss: 1.6920 - val_acc: 0.6680\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6606 - acc: 0.6816 - val_loss: 1.6573 - val_acc: 0.6790\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6275 - acc: 0.6880 - val_loss: 1.6284 - val_acc: 0.6850\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5967 - acc: 0.6965 - val_loss: 1.6004 - val_acc: 0.6910\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5678 - acc: 0.7033 - val_loss: 1.5743 - val_acc: 0.6980\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5410 - acc: 0.7073 - val_loss: 1.5470 - val_acc: 0.7010\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5161 - acc: 0.7149 - val_loss: 1.5244 - val_acc: 0.7040\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4926 - acc: 0.7209 - val_loss: 1.5053 - val_acc: 0.7080\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4708 - acc: 0.7243 - val_loss: 1.4829 - val_acc: 0.7140\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4502 - acc: 0.7295 - val_loss: 1.4670 - val_acc: 0.7140\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4306 - acc: 0.7303 - val_loss: 1.4505 - val_acc: 0.7190\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4130 - acc: 0.7364 - val_loss: 1.4339 - val_acc: 0.7220\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3955 - acc: 0.7399 - val_loss: 1.4207 - val_acc: 0.7190\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3794 - acc: 0.7431 - val_loss: 1.4074 - val_acc: 0.7290\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3645 - acc: 0.7467 - val_loss: 1.3934 - val_acc: 0.7280\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3497 - acc: 0.7465 - val_loss: 1.3825 - val_acc: 0.7290\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3357 - acc: 0.7532 - val_loss: 1.3686 - val_acc: 0.7280\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3225 - acc: 0.7543 - val_loss: 1.3591 - val_acc: 0.7340\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3097 - acc: 0.7581 - val_loss: 1.3473 - val_acc: 0.7390\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2976 - acc: 0.7611 - val_loss: 1.3396 - val_acc: 0.7360\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2859 - acc: 0.7635 - val_loss: 1.3280 - val_acc: 0.7360\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2752 - acc: 0.7644 - val_loss: 1.3192 - val_acc: 0.7470\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2640 - acc: 0.7704 - val_loss: 1.3126 - val_acc: 0.7430\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2536 - acc: 0.7721 - val_loss: 1.3033 - val_acc: 0.7410\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2431 - acc: 0.7748 - val_loss: 1.2955 - val_acc: 0.7450\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2338 - acc: 0.7768 - val_loss: 1.2890 - val_acc: 0.7450\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2242 - acc: 0.7819 - val_loss: 1.2814 - val_acc: 0.7470\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2151 - acc: 0.7839 - val_loss: 1.2749 - val_acc: 0.7530\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2061 - acc: 0.7853 - val_loss: 1.2705 - val_acc: 0.7530\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1976 - acc: 0.7868 - val_loss: 1.2605 - val_acc: 0.7530\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1892 - acc: 0.7880 - val_loss: 1.2570 - val_acc: 0.7500\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1816 - acc: 0.7935 - val_loss: 1.2523 - val_acc: 0.7570\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1728 - acc: 0.7903 - val_loss: 1.2415 - val_acc: 0.7560\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1652 - acc: 0.7979 - val_loss: 1.2407 - val_acc: 0.7530\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1581 - acc: 0.7996 - val_loss: 1.2323 - val_acc: 0.7570\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1503 - acc: 0.8021 - val_loss: 1.2269 - val_acc: 0.7560\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1434 - acc: 0.8017 - val_loss: 1.2257 - val_acc: 0.7540\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1365 - acc: 0.8061 - val_loss: 1.2192 - val_acc: 0.7600\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1295 - acc: 0.8060 - val_loss: 1.2148 - val_acc: 0.7630\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1228 - acc: 0.8063 - val_loss: 1.2080 - val_acc: 0.7600\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1161 - acc: 0.8111 - val_loss: 1.2063 - val_acc: 0.7600\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1096 - acc: 0.8117 - val_loss: 1.2008 - val_acc: 0.7640\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1030 - acc: 0.8121 - val_loss: 1.1977 - val_acc: 0.7570\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0970 - acc: 0.8164 - val_loss: 1.1898 - val_acc: 0.7590\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0909 - acc: 0.8169 - val_loss: 1.1879 - val_acc: 0.7620\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0844 - acc: 0.8191 - val_loss: 1.1841 - val_acc: 0.7610\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0786 - acc: 0.8189 - val_loss: 1.1802 - val_acc: 0.7560\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0729 - acc: 0.8217 - val_loss: 1.1791 - val_acc: 0.7650\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0673 - acc: 0.8212 - val_loss: 1.1738 - val_acc: 0.7530\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0614 - acc: 0.8227 - val_loss: 1.1694 - val_acc: 0.7700\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0554 - acc: 0.8228 - val_loss: 1.1670 - val_acc: 0.7610\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0504 - acc: 0.8275 - val_loss: 1.1605 - val_acc: 0.7610\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0447 - acc: 0.8279 - val_loss: 1.1578 - val_acc: 0.7570\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0395 - acc: 0.8288 - val_loss: 1.1535 - val_acc: 0.7620\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0341 - acc: 0.8303 - val_loss: 1.1495 - val_acc: 0.7620\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0291 - acc: 0.8312 - val_loss: 1.1503 - val_acc: 0.7630\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0237 - acc: 0.8315 - val_loss: 1.1476 - val_acc: 0.7580\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0189 - acc: 0.8333 - val_loss: 1.1437 - val_acc: 0.7660\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0135 - acc: 0.8341 - val_loss: 1.1405 - val_acc: 0.7620\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0088 - acc: 0.8353 - val_loss: 1.1409 - val_acc: 0.7570\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0041 - acc: 0.8380 - val_loss: 1.1348 - val_acc: 0.7590\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9991 - acc: 0.8379 - val_loss: 1.1341 - val_acc: 0.7670\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9946 - acc: 0.8396 - val_loss: 1.1278 - val_acc: 0.7630\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9896 - acc: 0.8411 - val_loss: 1.1263 - val_acc: 0.7650\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9848 - acc: 0.8432 - val_loss: 1.1240 - val_acc: 0.7670\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9804 - acc: 0.8432 - val_loss: 1.1205 - val_acc: 0.7670\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9760 - acc: 0.8427 - val_loss: 1.1178 - val_acc: 0.7600\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9716 - acc: 0.8459 - val_loss: 1.1173 - val_acc: 0.7630\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9668 - acc: 0.8461 - val_loss: 1.1130 - val_acc: 0.7650\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9626 - acc: 0.8467 - val_loss: 1.1134 - val_acc: 0.7610\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9583 - acc: 0.8481 - val_loss: 1.1064 - val_acc: 0.7730\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9540 - acc: 0.8507 - val_loss: 1.1071 - val_acc: 0.7640\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9495 - acc: 0.8495 - val_loss: 1.1048 - val_acc: 0.7690\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9453 - acc: 0.8509 - val_loss: 1.1028 - val_acc: 0.7650\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9413 - acc: 0.8517 - val_loss: 1.0975 - val_acc: 0.7690\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9370 - acc: 0.8536 - val_loss: 1.0992 - val_acc: 0.7640\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9333 - acc: 0.8547 - val_loss: 1.0959 - val_acc: 0.7630\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9291 - acc: 0.8540 - val_loss: 1.0926 - val_acc: 0.7660\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9249 - acc: 0.8572 - val_loss: 1.0925 - val_acc: 0.7670\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9215 - acc: 0.8575 - val_loss: 1.0863 - val_acc: 0.7670\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9171 - acc: 0.8580 - val_loss: 1.0878 - val_acc: 0.7670\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9131 - acc: 0.8587 - val_loss: 1.0845 - val_acc: 0.7750\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9091 - acc: 0.8613 - val_loss: 1.0812 - val_acc: 0.7730\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9053 - acc: 0.8605 - val_loss: 1.0782 - val_acc: 0.7700\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9016 - acc: 0.8637 - val_loss: 1.0827 - val_acc: 0.7660\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8979 - acc: 0.8624 - val_loss: 1.0778 - val_acc: 0.7680\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8941 - acc: 0.8651 - val_loss: 1.0718 - val_acc: 0.7700\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8901 - acc: 0.8648 - val_loss: 1.0746 - val_acc: 0.7650\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8869 - acc: 0.8659 - val_loss: 1.0729 - val_acc: 0.7660\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8833 - acc: 0.8679 - val_loss: 1.0677 - val_acc: 0.7680\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8799 - acc: 0.8684 - val_loss: 1.0665 - val_acc: 0.7730\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8761 - acc: 0.8695 - val_loss: 1.0650 - val_acc: 0.7690\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8721 - acc: 0.8708 - val_loss: 1.0620 - val_acc: 0.7680\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8690 - acc: 0.8701 - val_loss: 1.0606 - val_acc: 0.7680\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8655 - acc: 0.8715 - val_loss: 1.0602 - val_acc: 0.7690\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8619 - acc: 0.8725 - val_loss: 1.0580 - val_acc: 0.7750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8589 - acc: 0.8731 - val_loss: 1.0615 - val_acc: 0.7630\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 15.9680 - acc: 0.1393 - val_loss: 15.5656 - val_acc: 0.1530\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 15.2071 - acc: 0.1741 - val_loss: 14.8233 - val_acc: 0.1850\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 14.4743 - acc: 0.2037 - val_loss: 14.1037 - val_acc: 0.2140\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 13.7628 - acc: 0.2236 - val_loss: 13.4044 - val_acc: 0.2340\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 13.0714 - acc: 0.2451 - val_loss: 12.7238 - val_acc: 0.2600\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 12.3981 - acc: 0.2689 - val_loss: 12.0603 - val_acc: 0.2800\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.7417 - acc: 0.3036 - val_loss: 11.4143 - val_acc: 0.3020\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 11.1028 - acc: 0.3436 - val_loss: 10.7868 - val_acc: 0.3510\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 10.4825 - acc: 0.3916 - val_loss: 10.1781 - val_acc: 0.3780\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.8825 - acc: 0.4228 - val_loss: 9.5912 - val_acc: 0.4100\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.3036 - acc: 0.4568 - val_loss: 9.0249 - val_acc: 0.4380\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 8.7467 - acc: 0.4855 - val_loss: 8.4808 - val_acc: 0.4640\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 8.2114 - acc: 0.5143 - val_loss: 7.9591 - val_acc: 0.5040\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 7.6986 - acc: 0.5381 - val_loss: 7.4579 - val_acc: 0.5250\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 7.2082 - acc: 0.5596 - val_loss: 6.9803 - val_acc: 0.5470\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 6.7406 - acc: 0.5820 - val_loss: 6.5226 - val_acc: 0.5670\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 6.2947 - acc: 0.5988 - val_loss: 6.0885 - val_acc: 0.6000\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.8723 - acc: 0.6245 - val_loss: 5.6768 - val_acc: 0.6150\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.4737 - acc: 0.6363 - val_loss: 5.2910 - val_acc: 0.6160\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 5.0983 - acc: 0.6476 - val_loss: 4.9287 - val_acc: 0.6300\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.7465 - acc: 0.6621 - val_loss: 4.5875 - val_acc: 0.6360\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.4184 - acc: 0.6693 - val_loss: 4.2705 - val_acc: 0.6410\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.1134 - acc: 0.6789 - val_loss: 3.9765 - val_acc: 0.6550\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.8302 - acc: 0.6801 - val_loss: 3.7051 - val_acc: 0.6610\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.5699 - acc: 0.6885 - val_loss: 3.4564 - val_acc: 0.6680\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.3321 - acc: 0.6912 - val_loss: 3.2273 - val_acc: 0.6800\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.1161 - acc: 0.6943 - val_loss: 3.0209 - val_acc: 0.6930\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.9227 - acc: 0.6964 - val_loss: 2.8430 - val_acc: 0.6880\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.7504 - acc: 0.6972 - val_loss: 2.6779 - val_acc: 0.6910\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.5991 - acc: 0.6972 - val_loss: 2.5368 - val_acc: 0.6920\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.4679 - acc: 0.6991 - val_loss: 2.4142 - val_acc: 0.6950\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.3570 - acc: 0.6971 - val_loss: 2.3140 - val_acc: 0.6910\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.2651 - acc: 0.6953 - val_loss: 2.2299 - val_acc: 0.6940\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.1903 - acc: 0.6976 - val_loss: 2.1631 - val_acc: 0.6970\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1324 - acc: 0.6955 - val_loss: 2.1106 - val_acc: 0.6960\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0879 - acc: 0.6968 - val_loss: 2.0724 - val_acc: 0.6950\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0542 - acc: 0.6953 - val_loss: 2.0474 - val_acc: 0.6850\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0274 - acc: 0.6952 - val_loss: 2.0159 - val_acc: 0.6950\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0035 - acc: 0.6959 - val_loss: 1.9952 - val_acc: 0.6880\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9826 - acc: 0.6937 - val_loss: 1.9762 - val_acc: 0.6870\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9633 - acc: 0.6939 - val_loss: 1.9531 - val_acc: 0.6940\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9450 - acc: 0.6952 - val_loss: 1.9379 - val_acc: 0.6880\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9276 - acc: 0.6951 - val_loss: 1.9178 - val_acc: 0.6970\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9115 - acc: 0.6952 - val_loss: 1.9026 - val_acc: 0.6960\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8956 - acc: 0.6955 - val_loss: 1.8880 - val_acc: 0.6890\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8807 - acc: 0.6968 - val_loss: 1.8734 - val_acc: 0.6970\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8661 - acc: 0.6951 - val_loss: 1.8583 - val_acc: 0.6930\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8517 - acc: 0.6956 - val_loss: 1.8419 - val_acc: 0.7020\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8385 - acc: 0.6964 - val_loss: 1.8311 - val_acc: 0.6920\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8257 - acc: 0.6971 - val_loss: 1.8162 - val_acc: 0.7010\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8127 - acc: 0.6949 - val_loss: 1.8029 - val_acc: 0.6960\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8003 - acc: 0.6963 - val_loss: 1.7939 - val_acc: 0.6970\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7883 - acc: 0.6980 - val_loss: 1.7802 - val_acc: 0.6930\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7768 - acc: 0.6953 - val_loss: 1.7670 - val_acc: 0.6960\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7652 - acc: 0.6963 - val_loss: 1.7544 - val_acc: 0.6970\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7537 - acc: 0.6987 - val_loss: 1.7486 - val_acc: 0.7020\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7431 - acc: 0.6975 - val_loss: 1.7374 - val_acc: 0.6980\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7327 - acc: 0.6968 - val_loss: 1.7239 - val_acc: 0.6930\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7220 - acc: 0.6992 - val_loss: 1.7144 - val_acc: 0.6930\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7126 - acc: 0.6993 - val_loss: 1.7053 - val_acc: 0.7020\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7027 - acc: 0.6997 - val_loss: 1.6931 - val_acc: 0.7060\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6928 - acc: 0.6984 - val_loss: 1.6823 - val_acc: 0.7020\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6834 - acc: 0.6993 - val_loss: 1.6742 - val_acc: 0.7010\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6743 - acc: 0.7001 - val_loss: 1.6653 - val_acc: 0.6920\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6648 - acc: 0.7000 - val_loss: 1.6541 - val_acc: 0.7000\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6561 - acc: 0.7012 - val_loss: 1.6478 - val_acc: 0.6990\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6474 - acc: 0.7009 - val_loss: 1.6431 - val_acc: 0.7010\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6388 - acc: 0.7004 - val_loss: 1.6280 - val_acc: 0.7060\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6304 - acc: 0.7008 - val_loss: 1.6239 - val_acc: 0.7000\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6219 - acc: 0.7021 - val_loss: 1.6132 - val_acc: 0.7100\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6141 - acc: 0.7033 - val_loss: 1.6022 - val_acc: 0.7080\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6060 - acc: 0.7021 - val_loss: 1.5958 - val_acc: 0.7060\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5980 - acc: 0.7017 - val_loss: 1.5879 - val_acc: 0.7060\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5901 - acc: 0.7057 - val_loss: 1.5783 - val_acc: 0.7120\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5824 - acc: 0.7039 - val_loss: 1.5708 - val_acc: 0.7130\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5750 - acc: 0.7032 - val_loss: 1.5689 - val_acc: 0.7040\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5681 - acc: 0.7029 - val_loss: 1.5563 - val_acc: 0.7180\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5601 - acc: 0.7063 - val_loss: 1.5474 - val_acc: 0.7150\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5531 - acc: 0.7040 - val_loss: 1.5420 - val_acc: 0.7120\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5459 - acc: 0.7051 - val_loss: 1.5356 - val_acc: 0.7110\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5384 - acc: 0.7044 - val_loss: 1.5267 - val_acc: 0.7180\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5317 - acc: 0.7057 - val_loss: 1.5222 - val_acc: 0.7200\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5257 - acc: 0.7069 - val_loss: 1.5162 - val_acc: 0.7100\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5177 - acc: 0.7071 - val_loss: 1.5115 - val_acc: 0.7090\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5110 - acc: 0.7060 - val_loss: 1.5054 - val_acc: 0.7190\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5044 - acc: 0.7080 - val_loss: 1.4934 - val_acc: 0.7130\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4977 - acc: 0.7072 - val_loss: 1.4869 - val_acc: 0.7160\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4911 - acc: 0.7067 - val_loss: 1.4809 - val_acc: 0.7120\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4851 - acc: 0.7080 - val_loss: 1.4752 - val_acc: 0.7180\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4784 - acc: 0.7099 - val_loss: 1.4667 - val_acc: 0.7220\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4726 - acc: 0.7101 - val_loss: 1.4654 - val_acc: 0.7110\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4664 - acc: 0.7104 - val_loss: 1.4554 - val_acc: 0.7150\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4599 - acc: 0.7103 - val_loss: 1.4524 - val_acc: 0.7150\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4538 - acc: 0.7116 - val_loss: 1.4406 - val_acc: 0.7240\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4480 - acc: 0.7109 - val_loss: 1.4346 - val_acc: 0.7270\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4413 - acc: 0.7103 - val_loss: 1.4295 - val_acc: 0.7220\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4356 - acc: 0.7108 - val_loss: 1.4242 - val_acc: 0.7180\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4300 - acc: 0.7120 - val_loss: 1.4204 - val_acc: 0.7220\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4249 - acc: 0.7139 - val_loss: 1.4131 - val_acc: 0.7170\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4184 - acc: 0.7153 - val_loss: 1.4199 - val_acc: 0.7090\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4136 - acc: 0.7125 - val_loss: 1.4067 - val_acc: 0.7100\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4069 - acc: 0.7144 - val_loss: 1.4011 - val_acc: 0.7170\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4023 - acc: 0.7141 - val_loss: 1.3885 - val_acc: 0.7200\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3964 - acc: 0.7141 - val_loss: 1.3894 - val_acc: 0.7140\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3913 - acc: 0.7151 - val_loss: 1.3879 - val_acc: 0.7240\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3860 - acc: 0.7145 - val_loss: 1.3735 - val_acc: 0.7170\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3805 - acc: 0.7152 - val_loss: 1.3702 - val_acc: 0.7210\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3753 - acc: 0.7169 - val_loss: 1.3699 - val_acc: 0.7160\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3707 - acc: 0.7157 - val_loss: 1.3565 - val_acc: 0.7180\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3651 - acc: 0.7155 - val_loss: 1.3515 - val_acc: 0.7210\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3602 - acc: 0.7171 - val_loss: 1.3524 - val_acc: 0.7240\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3552 - acc: 0.7169 - val_loss: 1.3466 - val_acc: 0.7170\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3504 - acc: 0.7172 - val_loss: 1.3402 - val_acc: 0.7200\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3453 - acc: 0.7172 - val_loss: 1.3358 - val_acc: 0.7250\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3406 - acc: 0.7187 - val_loss: 1.3315 - val_acc: 0.7230\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3362 - acc: 0.7195 - val_loss: 1.3229 - val_acc: 0.7240\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3313 - acc: 0.7199 - val_loss: 1.3213 - val_acc: 0.7200\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3269 - acc: 0.7187 - val_loss: 1.3183 - val_acc: 0.7190\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3219 - acc: 0.7209 - val_loss: 1.3069 - val_acc: 0.7220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3173 - acc: 0.7212 - val_loss: 1.3061 - val_acc: 0.7250\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 15.9723 - acc: 0.1727 - val_loss: 15.5643 - val_acc: 0.1900\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 15.2128 - acc: 0.1935 - val_loss: 14.8200 - val_acc: 0.2140\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 14.4748 - acc: 0.2201 - val_loss: 14.0951 - val_acc: 0.2290\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 13.7559 - acc: 0.2485 - val_loss: 13.3895 - val_acc: 0.2490\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 13.0563 - acc: 0.2760 - val_loss: 12.7035 - val_acc: 0.2700\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 12.3757 - acc: 0.3051 - val_loss: 12.0346 - val_acc: 0.2870\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.7128 - acc: 0.3297 - val_loss: 11.3838 - val_acc: 0.3230\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.0692 - acc: 0.3615 - val_loss: 10.7516 - val_acc: 0.3440\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 10.4471 - acc: 0.3851 - val_loss: 10.1427 - val_acc: 0.3690\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 9.8462 - acc: 0.4033 - val_loss: 9.5543 - val_acc: 0.4000\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 9.2676 - acc: 0.4328 - val_loss: 8.9883 - val_acc: 0.4210\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 8.7123 - acc: 0.4548 - val_loss: 8.4491 - val_acc: 0.4380\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 8.1797 - acc: 0.4767 - val_loss: 7.9287 - val_acc: 0.4770\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 7.6698 - acc: 0.5012 - val_loss: 7.4317 - val_acc: 0.4810\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 7.1822 - acc: 0.5148 - val_loss: 6.9555 - val_acc: 0.4870\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 6.7171 - acc: 0.5316 - val_loss: 6.5029 - val_acc: 0.5010\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 6.2748 - acc: 0.5428 - val_loss: 6.0725 - val_acc: 0.5080\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.8557 - acc: 0.5543 - val_loss: 5.6659 - val_acc: 0.5220\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.4588 - acc: 0.5676 - val_loss: 5.2805 - val_acc: 0.5310\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.0844 - acc: 0.5759 - val_loss: 4.9169 - val_acc: 0.5360\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.7329 - acc: 0.5864 - val_loss: 4.5784 - val_acc: 0.5460\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 4.4046 - acc: 0.5935 - val_loss: 4.2631 - val_acc: 0.5600\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.0986 - acc: 0.6011 - val_loss: 3.9689 - val_acc: 0.5750\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.8146 - acc: 0.6087 - val_loss: 3.6944 - val_acc: 0.5880\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.5530 - acc: 0.6139 - val_loss: 3.4425 - val_acc: 0.5860\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 3.3137 - acc: 0.6227 - val_loss: 3.2166 - val_acc: 0.5930\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 3.0968 - acc: 0.6267 - val_loss: 3.0082 - val_acc: 0.6050\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.9013 - acc: 0.6341 - val_loss: 2.8231 - val_acc: 0.6040\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.7271 - acc: 0.6367 - val_loss: 2.6596 - val_acc: 0.6140\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.5747 - acc: 0.6401 - val_loss: 2.5191 - val_acc: 0.6150\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.4423 - acc: 0.6427 - val_loss: 2.3976 - val_acc: 0.6200\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.3311 - acc: 0.6448 - val_loss: 2.2938 - val_acc: 0.6220\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.2385 - acc: 0.6461 - val_loss: 2.2124 - val_acc: 0.6300\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1646 - acc: 0.6461 - val_loss: 2.1480 - val_acc: 0.6160\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1074 - acc: 0.6468 - val_loss: 2.0958 - val_acc: 0.6320\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0647 - acc: 0.6495 - val_loss: 2.0590 - val_acc: 0.6310\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0331 - acc: 0.6501 - val_loss: 2.0319 - val_acc: 0.6360\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0082 - acc: 0.6512 - val_loss: 2.0088 - val_acc: 0.6400\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9865 - acc: 0.6551 - val_loss: 1.9856 - val_acc: 0.6400\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9668 - acc: 0.6532 - val_loss: 1.9660 - val_acc: 0.6410\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9486 - acc: 0.6561 - val_loss: 1.9511 - val_acc: 0.6410\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9313 - acc: 0.6564 - val_loss: 1.9309 - val_acc: 0.6460\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9152 - acc: 0.6573 - val_loss: 1.9145 - val_acc: 0.6430\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8996 - acc: 0.6609 - val_loss: 1.9061 - val_acc: 0.6460\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8850 - acc: 0.6613 - val_loss: 1.8910 - val_acc: 0.6460\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8709 - acc: 0.6589 - val_loss: 1.8709 - val_acc: 0.6560\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8569 - acc: 0.6616 - val_loss: 1.8595 - val_acc: 0.6530\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8440 - acc: 0.6624 - val_loss: 1.8495 - val_acc: 0.6520\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8313 - acc: 0.6640 - val_loss: 1.8333 - val_acc: 0.6540\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8188 - acc: 0.6647 - val_loss: 1.8191 - val_acc: 0.6590\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8069 - acc: 0.6660 - val_loss: 1.8084 - val_acc: 0.6530\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7952 - acc: 0.6664 - val_loss: 1.7976 - val_acc: 0.6610\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7839 - acc: 0.6680 - val_loss: 1.7850 - val_acc: 0.6580\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7725 - acc: 0.6713 - val_loss: 1.7764 - val_acc: 0.6620\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7618 - acc: 0.6708 - val_loss: 1.7611 - val_acc: 0.6610\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7507 - acc: 0.6723 - val_loss: 1.7555 - val_acc: 0.6580\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7405 - acc: 0.6719 - val_loss: 1.7460 - val_acc: 0.6650\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7303 - acc: 0.6716 - val_loss: 1.7340 - val_acc: 0.6590\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7198 - acc: 0.6743 - val_loss: 1.7241 - val_acc: 0.6600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7102 - acc: 0.6732 - val_loss: 1.7137 - val_acc: 0.6680\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7002 - acc: 0.6753 - val_loss: 1.7007 - val_acc: 0.6650\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6913 - acc: 0.6759 - val_loss: 1.6955 - val_acc: 0.6690\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6818 - acc: 0.6764 - val_loss: 1.6841 - val_acc: 0.6680\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6723 - acc: 0.6783 - val_loss: 1.6763 - val_acc: 0.6740\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6635 - acc: 0.6783 - val_loss: 1.6633 - val_acc: 0.6700\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6533 - acc: 0.6801 - val_loss: 1.6535 - val_acc: 0.6700\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6447 - acc: 0.6804 - val_loss: 1.6448 - val_acc: 0.6740\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6356 - acc: 0.6812 - val_loss: 1.6365 - val_acc: 0.6770\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6271 - acc: 0.6828 - val_loss: 1.6290 - val_acc: 0.6760\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6188 - acc: 0.6827 - val_loss: 1.6221 - val_acc: 0.6760\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6105 - acc: 0.6825 - val_loss: 1.6103 - val_acc: 0.6770\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6017 - acc: 0.6839 - val_loss: 1.6035 - val_acc: 0.6790\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5935 - acc: 0.6856 - val_loss: 1.5962 - val_acc: 0.6800\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5852 - acc: 0.6856 - val_loss: 1.5893 - val_acc: 0.6770\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5772 - acc: 0.6857 - val_loss: 1.5765 - val_acc: 0.6830\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5692 - acc: 0.6869 - val_loss: 1.5699 - val_acc: 0.6830\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5618 - acc: 0.6880 - val_loss: 1.5630 - val_acc: 0.6840\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5536 - acc: 0.6876 - val_loss: 1.5547 - val_acc: 0.6880\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5458 - acc: 0.6889 - val_loss: 1.5453 - val_acc: 0.6900\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5389 - acc: 0.6895 - val_loss: 1.5375 - val_acc: 0.6900\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5312 - acc: 0.6892 - val_loss: 1.5295 - val_acc: 0.6910\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5234 - acc: 0.6901 - val_loss: 1.5222 - val_acc: 0.6880\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5166 - acc: 0.6900 - val_loss: 1.5156 - val_acc: 0.6920\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5087 - acc: 0.6913 - val_loss: 1.5210 - val_acc: 0.6930\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5021 - acc: 0.6912 - val_loss: 1.4998 - val_acc: 0.6930\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4946 - acc: 0.6933 - val_loss: 1.4911 - val_acc: 0.6920\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4877 - acc: 0.6931 - val_loss: 1.4842 - val_acc: 0.6930\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4809 - acc: 0.6941 - val_loss: 1.4768 - val_acc: 0.6950\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4738 - acc: 0.6945 - val_loss: 1.4740 - val_acc: 0.6980\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4670 - acc: 0.6944 - val_loss: 1.4667 - val_acc: 0.6950\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4603 - acc: 0.6965 - val_loss: 1.4583 - val_acc: 0.6980\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4538 - acc: 0.6963 - val_loss: 1.4520 - val_acc: 0.7050\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4471 - acc: 0.6965 - val_loss: 1.4435 - val_acc: 0.7010\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4407 - acc: 0.6956 - val_loss: 1.4383 - val_acc: 0.7000\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4343 - acc: 0.6961 - val_loss: 1.4322 - val_acc: 0.7000\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4282 - acc: 0.6976 - val_loss: 1.4290 - val_acc: 0.7030\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4216 - acc: 0.6985 - val_loss: 1.4188 - val_acc: 0.7080\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4157 - acc: 0.6983 - val_loss: 1.4167 - val_acc: 0.7090\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4093 - acc: 0.6993 - val_loss: 1.4132 - val_acc: 0.7050\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4038 - acc: 0.6980 - val_loss: 1.4003 - val_acc: 0.7040\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3971 - acc: 0.6991 - val_loss: 1.3956 - val_acc: 0.7080\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3909 - acc: 0.7000 - val_loss: 1.3878 - val_acc: 0.7110\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3850 - acc: 0.6996 - val_loss: 1.3826 - val_acc: 0.7080\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3789 - acc: 0.7019 - val_loss: 1.3785 - val_acc: 0.7140\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3736 - acc: 0.7003 - val_loss: 1.3699 - val_acc: 0.7130\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3675 - acc: 0.7012 - val_loss: 1.3657 - val_acc: 0.7140\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3616 - acc: 0.7013 - val_loss: 1.3627 - val_acc: 0.7080\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3563 - acc: 0.7027 - val_loss: 1.3530 - val_acc: 0.7150\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3506 - acc: 0.7012 - val_loss: 1.3530 - val_acc: 0.7180\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3454 - acc: 0.7025 - val_loss: 1.3431 - val_acc: 0.7120\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3399 - acc: 0.7040 - val_loss: 1.3367 - val_acc: 0.7170\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3347 - acc: 0.7027 - val_loss: 1.3342 - val_acc: 0.7160\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3293 - acc: 0.7049 - val_loss: 1.3333 - val_acc: 0.7210\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3239 - acc: 0.7037 - val_loss: 1.3270 - val_acc: 0.7230\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3192 - acc: 0.7045 - val_loss: 1.3247 - val_acc: 0.7160\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3138 - acc: 0.7056 - val_loss: 1.3108 - val_acc: 0.7180\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3087 - acc: 0.7044 - val_loss: 1.3052 - val_acc: 0.7210\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3038 - acc: 0.7052 - val_loss: 1.3004 - val_acc: 0.7230\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2983 - acc: 0.7055 - val_loss: 1.2954 - val_acc: 0.7230\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2931 - acc: 0.7076 - val_loss: 1.2894 - val_acc: 0.7210\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2883 - acc: 0.7071 - val_loss: 1.2853 - val_acc: 0.7210\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2840 - acc: 0.7063 - val_loss: 1.2861 - val_acc: 0.7200\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2791 - acc: 0.7071 - val_loss: 1.2898 - val_acc: 0.7200\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2749 - acc: 0.7093 - val_loss: 1.2699 - val_acc: 0.7230\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2698 - acc: 0.7089 - val_loss: 1.2713 - val_acc: 0.7210\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2648 - acc: 0.7101 - val_loss: 1.2659 - val_acc: 0.7210\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2604 - acc: 0.7096 - val_loss: 1.2611 - val_acc: 0.7210\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2561 - acc: 0.7093 - val_loss: 1.2560 - val_acc: 0.7220\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2516 - acc: 0.7079 - val_loss: 1.2472 - val_acc: 0.7200\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2464 - acc: 0.7089 - val_loss: 1.2503 - val_acc: 0.7260\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2433 - acc: 0.7079 - val_loss: 1.2463 - val_acc: 0.7220\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2386 - acc: 0.7103 - val_loss: 1.2380 - val_acc: 0.7220\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2342 - acc: 0.7119 - val_loss: 1.2348 - val_acc: 0.7240\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2296 - acc: 0.7119 - val_loss: 1.2292 - val_acc: 0.7260\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2259 - acc: 0.7109 - val_loss: 1.2250 - val_acc: 0.7230\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2213 - acc: 0.7131 - val_loss: 1.2215 - val_acc: 0.7200\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2175 - acc: 0.7107 - val_loss: 1.2171 - val_acc: 0.7220\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2136 - acc: 0.7144 - val_loss: 1.2178 - val_acc: 0.7230\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2102 - acc: 0.7141 - val_loss: 1.2080 - val_acc: 0.7190\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2064 - acc: 0.7140 - val_loss: 1.2072 - val_acc: 0.7230\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2021 - acc: 0.7141 - val_loss: 1.2047 - val_acc: 0.7220\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1987 - acc: 0.7141 - val_loss: 1.1995 - val_acc: 0.7280\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1952 - acc: 0.7144 - val_loss: 1.1925 - val_acc: 0.7240\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1913 - acc: 0.7153 - val_loss: 1.1889 - val_acc: 0.7240\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1877 - acc: 0.7160 - val_loss: 1.1927 - val_acc: 0.7250\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1845 - acc: 0.7163 - val_loss: 1.1878 - val_acc: 0.7250\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1808 - acc: 0.7169 - val_loss: 1.1814 - val_acc: 0.7240\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1775 - acc: 0.7177 - val_loss: 1.1766 - val_acc: 0.7250\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1741 - acc: 0.7180 - val_loss: 1.1833 - val_acc: 0.7270\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1711 - acc: 0.7192 - val_loss: 1.1692 - val_acc: 0.7300\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1675 - acc: 0.7183 - val_loss: 1.1741 - val_acc: 0.7300\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1646 - acc: 0.7188 - val_loss: 1.1633 - val_acc: 0.7240\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1611 - acc: 0.7196 - val_loss: 1.1618 - val_acc: 0.7250\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1583 - acc: 0.7177 - val_loss: 1.1586 - val_acc: 0.7260\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1551 - acc: 0.7183 - val_loss: 1.1605 - val_acc: 0.7280\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1527 - acc: 0.7179 - val_loss: 1.1556 - val_acc: 0.7260\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1497 - acc: 0.7180 - val_loss: 1.1482 - val_acc: 0.7290\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1463 - acc: 0.7187 - val_loss: 1.1471 - val_acc: 0.7300\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1432 - acc: 0.7193 - val_loss: 1.1420 - val_acc: 0.7320\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1407 - acc: 0.7197 - val_loss: 1.1419 - val_acc: 0.7310\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1364 - acc: 0.7203 - val_loss: 1.1461 - val_acc: 0.7320\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1352 - acc: 0.7215 - val_loss: 1.1398 - val_acc: 0.7350\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1324 - acc: 0.7208 - val_loss: 1.1463 - val_acc: 0.7270\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1305 - acc: 0.7193 - val_loss: 1.1361 - val_acc: 0.7260\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1269 - acc: 0.7213 - val_loss: 1.1308 - val_acc: 0.7270\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1245 - acc: 0.7220 - val_loss: 1.1329 - val_acc: 0.7280\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1228 - acc: 0.7221 - val_loss: 1.1263 - val_acc: 0.7280\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1195 - acc: 0.7225 - val_loss: 1.1258 - val_acc: 0.7290\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1169 - acc: 0.7213 - val_loss: 1.1303 - val_acc: 0.7240\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1146 - acc: 0.7237 - val_loss: 1.1196 - val_acc: 0.7300\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1124 - acc: 0.7237 - val_loss: 1.1254 - val_acc: 0.7290\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1111 - acc: 0.7243 - val_loss: 1.1171 - val_acc: 0.7320\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1078 - acc: 0.7224 - val_loss: 1.1141 - val_acc: 0.7290\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1062 - acc: 0.7224 - val_loss: 1.1122 - val_acc: 0.7310\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1045 - acc: 0.7225 - val_loss: 1.1108 - val_acc: 0.7270\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1018 - acc: 0.7240 - val_loss: 1.1101 - val_acc: 0.7290\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1000 - acc: 0.7233 - val_loss: 1.1084 - val_acc: 0.7300\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0981 - acc: 0.7243 - val_loss: 1.1009 - val_acc: 0.7330\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0963 - acc: 0.7236 - val_loss: 1.0978 - val_acc: 0.7310\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0940 - acc: 0.7257 - val_loss: 1.1029 - val_acc: 0.7290\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0924 - acc: 0.7272 - val_loss: 1.0986 - val_acc: 0.7250\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0903 - acc: 0.7261 - val_loss: 1.0952 - val_acc: 0.7320\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0880 - acc: 0.7257 - val_loss: 1.0921 - val_acc: 0.7330\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0866 - acc: 0.7256 - val_loss: 1.0907 - val_acc: 0.7330\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0841 - acc: 0.7260 - val_loss: 1.0948 - val_acc: 0.7280\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0832 - acc: 0.7263 - val_loss: 1.0898 - val_acc: 0.7250\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0816 - acc: 0.7263 - val_loss: 1.0896 - val_acc: 0.7320\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0799 - acc: 0.7257 - val_loss: 1.0879 - val_acc: 0.7300\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0784 - acc: 0.7260 - val_loss: 1.0848 - val_acc: 0.7290\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0763 - acc: 0.7276 - val_loss: 1.0819 - val_acc: 0.7310\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0740 - acc: 0.7275 - val_loss: 1.0808 - val_acc: 0.7330\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0737 - acc: 0.7279 - val_loss: 1.0762 - val_acc: 0.7320\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0717 - acc: 0.7275 - val_loss: 1.0790 - val_acc: 0.7280\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0697 - acc: 0.7276 - val_loss: 1.0761 - val_acc: 0.7310\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0681 - acc: 0.7307 - val_loss: 1.0764 - val_acc: 0.7310\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0672 - acc: 0.7277 - val_loss: 1.0824 - val_acc: 0.7270\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0659 - acc: 0.7284 - val_loss: 1.0753 - val_acc: 0.7310\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0645 - acc: 0.7304 - val_loss: 1.0759 - val_acc: 0.7290\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0633 - acc: 0.7281 - val_loss: 1.0790 - val_acc: 0.7220\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0612 - acc: 0.7296 - val_loss: 1.0802 - val_acc: 0.7300\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0602 - acc: 0.7284 - val_loss: 1.0686 - val_acc: 0.7280\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0586 - acc: 0.7311 - val_loss: 1.0701 - val_acc: 0.7300\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0583 - acc: 0.7293 - val_loss: 1.0641 - val_acc: 0.7330\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0571 - acc: 0.7321 - val_loss: 1.0761 - val_acc: 0.7260\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0559 - acc: 0.7304 - val_loss: 1.0616 - val_acc: 0.7320\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0541 - acc: 0.7295 - val_loss: 1.0656 - val_acc: 0.7360\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0529 - acc: 0.7308 - val_loss: 1.0601 - val_acc: 0.7350\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0517 - acc: 0.7321 - val_loss: 1.0619 - val_acc: 0.7330\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0507 - acc: 0.7309 - val_loss: 1.0584 - val_acc: 0.7360\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0491 - acc: 0.7313 - val_loss: 1.0567 - val_acc: 0.7320\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0472 - acc: 0.7324 - val_loss: 1.0565 - val_acc: 0.7360\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0475 - acc: 0.7321 - val_loss: 1.0560 - val_acc: 0.7330\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0453 - acc: 0.7329 - val_loss: 1.0631 - val_acc: 0.7310\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0447 - acc: 0.7348 - val_loss: 1.0567 - val_acc: 0.7350\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0437 - acc: 0.7341 - val_loss: 1.0580 - val_acc: 0.7350\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0425 - acc: 0.7357 - val_loss: 1.0565 - val_acc: 0.7350\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0414 - acc: 0.7328 - val_loss: 1.0524 - val_acc: 0.7310\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0407 - acc: 0.7336 - val_loss: 1.0466 - val_acc: 0.7340\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0383 - acc: 0.7347 - val_loss: 1.0463 - val_acc: 0.7360\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0379 - acc: 0.7340 - val_loss: 1.0478 - val_acc: 0.7390\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0374 - acc: 0.7344 - val_loss: 1.0470 - val_acc: 0.7390\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0363 - acc: 0.7357 - val_loss: 1.0616 - val_acc: 0.7280\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0360 - acc: 0.7360 - val_loss: 1.0425 - val_acc: 0.7360\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0342 - acc: 0.7364 - val_loss: 1.0458 - val_acc: 0.7320\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0332 - acc: 0.7345 - val_loss: 1.0481 - val_acc: 0.7340\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0326 - acc: 0.7369 - val_loss: 1.0421 - val_acc: 0.7370\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0307 - acc: 0.7372 - val_loss: 1.0400 - val_acc: 0.7360\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0306 - acc: 0.7367 - val_loss: 1.0463 - val_acc: 0.7360\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0299 - acc: 0.7376 - val_loss: 1.0406 - val_acc: 0.7340\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0281 - acc: 0.7355 - val_loss: 1.0444 - val_acc: 0.7310\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0275 - acc: 0.7385 - val_loss: 1.0361 - val_acc: 0.7370\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0261 - acc: 0.7357 - val_loss: 1.0480 - val_acc: 0.7330\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0258 - acc: 0.7377 - val_loss: 1.0353 - val_acc: 0.7360\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0245 - acc: 0.7381 - val_loss: 1.0349 - val_acc: 0.7360\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0242 - acc: 0.7375 - val_loss: 1.0377 - val_acc: 0.7370\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0225 - acc: 0.7379 - val_loss: 1.0350 - val_acc: 0.7380\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0225 - acc: 0.7371 - val_loss: 1.0555 - val_acc: 0.7300\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0217 - acc: 0.7388 - val_loss: 1.0403 - val_acc: 0.7340\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0201 - acc: 0.7401 - val_loss: 1.0329 - val_acc: 0.7350\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0188 - acc: 0.7363 - val_loss: 1.0389 - val_acc: 0.7290\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0180 - acc: 0.7383 - val_loss: 1.0307 - val_acc: 0.7380\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0181 - acc: 0.7385 - val_loss: 1.0307 - val_acc: 0.7370\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0165 - acc: 0.7377 - val_loss: 1.0271 - val_acc: 0.7380\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0152 - acc: 0.7400 - val_loss: 1.0309 - val_acc: 0.7340\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0160 - acc: 0.7400 - val_loss: 1.0321 - val_acc: 0.7280\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0145 - acc: 0.7401 - val_loss: 1.0265 - val_acc: 0.7400\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0129 - acc: 0.7385 - val_loss: 1.0331 - val_acc: 0.7320\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0128 - acc: 0.7372 - val_loss: 1.0429 - val_acc: 0.7240\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0118 - acc: 0.7391 - val_loss: 1.0478 - val_acc: 0.7240\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0113 - acc: 0.7404 - val_loss: 1.0265 - val_acc: 0.7420\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0102 - acc: 0.7416 - val_loss: 1.0275 - val_acc: 0.7390\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0093 - acc: 0.7415 - val_loss: 1.0273 - val_acc: 0.7380\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0081 - acc: 0.7412 - val_loss: 1.0291 - val_acc: 0.7300\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0075 - acc: 0.7412 - val_loss: 1.0359 - val_acc: 0.7330\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0075 - acc: 0.7409 - val_loss: 1.0194 - val_acc: 0.7400\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0063 - acc: 0.7424 - val_loss: 1.0244 - val_acc: 0.7410\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0061 - acc: 0.7403 - val_loss: 1.0227 - val_acc: 0.7310\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0044 - acc: 0.7423 - val_loss: 1.0206 - val_acc: 0.7370\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0032 - acc: 0.7420 - val_loss: 1.0235 - val_acc: 0.7350\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0030 - acc: 0.7427 - val_loss: 1.0173 - val_acc: 0.7350\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0015 - acc: 0.7396 - val_loss: 1.0243 - val_acc: 0.7300\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0007 - acc: 0.7440 - val_loss: 1.0158 - val_acc: 0.7370\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0007 - acc: 0.7428 - val_loss: 1.0233 - val_acc: 0.7380\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9996 - acc: 0.7444 - val_loss: 1.0174 - val_acc: 0.7380\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9996 - acc: 0.7429 - val_loss: 1.0202 - val_acc: 0.7390\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9994 - acc: 0.7405 - val_loss: 1.0208 - val_acc: 0.7300\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9986 - acc: 0.7423 - val_loss: 1.0177 - val_acc: 0.7380\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9977 - acc: 0.7439 - val_loss: 1.0327 - val_acc: 0.7230\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9967 - acc: 0.7436 - val_loss: 1.0221 - val_acc: 0.7310\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9960 - acc: 0.7443 - val_loss: 1.0145 - val_acc: 0.7380\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9947 - acc: 0.7443 - val_loss: 1.0129 - val_acc: 0.7390\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9945 - acc: 0.7432 - val_loss: 1.0091 - val_acc: 0.7380\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9930 - acc: 0.7448 - val_loss: 1.0210 - val_acc: 0.7290\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9928 - acc: 0.7451 - val_loss: 1.0097 - val_acc: 0.7350\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9926 - acc: 0.7432 - val_loss: 1.0094 - val_acc: 0.7340\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9912 - acc: 0.7448 - val_loss: 1.0121 - val_acc: 0.7390\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9908 - acc: 0.7451 - val_loss: 1.0071 - val_acc: 0.7390\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9905 - acc: 0.7464 - val_loss: 1.0082 - val_acc: 0.7420\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9901 - acc: 0.7464 - val_loss: 1.0253 - val_acc: 0.7260\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9896 - acc: 0.7469 - val_loss: 1.0109 - val_acc: 0.7330\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9885 - acc: 0.7447 - val_loss: 1.0084 - val_acc: 0.7430\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9873 - acc: 0.7453 - val_loss: 1.0075 - val_acc: 0.7410\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9866 - acc: 0.7459 - val_loss: 1.0037 - val_acc: 0.7390\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9858 - acc: 0.7464 - val_loss: 1.0079 - val_acc: 0.7300\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9856 - acc: 0.7451 - val_loss: 1.0090 - val_acc: 0.7320\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9854 - acc: 0.7465 - val_loss: 1.0024 - val_acc: 0.7380\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9835 - acc: 0.7471 - val_loss: 1.0109 - val_acc: 0.7340\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9837 - acc: 0.7464 - val_loss: 1.0092 - val_acc: 0.7390\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9842 - acc: 0.7463 - val_loss: 1.0064 - val_acc: 0.7360\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9826 - acc: 0.7467 - val_loss: 1.0039 - val_acc: 0.7360\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9820 - acc: 0.7453 - val_loss: 1.0080 - val_acc: 0.7330\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9810 - acc: 0.7455 - val_loss: 1.0091 - val_acc: 0.7400\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9810 - acc: 0.7464 - val_loss: 1.0130 - val_acc: 0.7310\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9809 - acc: 0.7475 - val_loss: 1.0102 - val_acc: 0.7360\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9804 - acc: 0.7449 - val_loss: 1.0061 - val_acc: 0.7270\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9796 - acc: 0.7461 - val_loss: 1.0149 - val_acc: 0.7240\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9782 - acc: 0.7471 - val_loss: 0.9999 - val_acc: 0.7440\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9788 - acc: 0.7457 - val_loss: 1.0053 - val_acc: 0.7410\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9781 - acc: 0.7472 - val_loss: 0.9983 - val_acc: 0.7300\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9760 - acc: 0.7479 - val_loss: 1.0108 - val_acc: 0.7370\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9761 - acc: 0.7460 - val_loss: 0.9988 - val_acc: 0.7410\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9746 - acc: 0.7507 - val_loss: 1.0048 - val_acc: 0.7310\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9758 - acc: 0.7467 - val_loss: 1.0087 - val_acc: 0.7370\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9751 - acc: 0.7463 - val_loss: 0.9975 - val_acc: 0.7370\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9745 - acc: 0.7469 - val_loss: 1.0026 - val_acc: 0.7330\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9736 - acc: 0.7491 - val_loss: 1.0033 - val_acc: 0.7400\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9734 - acc: 0.7492 - val_loss: 1.0005 - val_acc: 0.7310\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9722 - acc: 0.7485 - val_loss: 0.9979 - val_acc: 0.7300\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9714 - acc: 0.7529 - val_loss: 0.9959 - val_acc: 0.7390\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9710 - acc: 0.7485 - val_loss: 0.9999 - val_acc: 0.7350\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9718 - acc: 0.7476 - val_loss: 1.0057 - val_acc: 0.7440\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9699 - acc: 0.7503 - val_loss: 0.9963 - val_acc: 0.7290\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9691 - acc: 0.7481 - val_loss: 0.9937 - val_acc: 0.7400\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9689 - acc: 0.7488 - val_loss: 1.0011 - val_acc: 0.7320\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9690 - acc: 0.7505 - val_loss: 1.0003 - val_acc: 0.7290\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9676 - acc: 0.7528 - val_loss: 0.9961 - val_acc: 0.7350\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9680 - acc: 0.7492 - val_loss: 0.9940 - val_acc: 0.7290\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9668 - acc: 0.7480 - val_loss: 0.9915 - val_acc: 0.7450\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9662 - acc: 0.7500 - val_loss: 1.0183 - val_acc: 0.7240\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9663 - acc: 0.7505 - val_loss: 0.9916 - val_acc: 0.7270\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9651 - acc: 0.7495 - val_loss: 0.9941 - val_acc: 0.7270\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9650 - acc: 0.7500 - val_loss: 0.9946 - val_acc: 0.7410\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9636 - acc: 0.7499 - val_loss: 1.0027 - val_acc: 0.7300\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9645 - acc: 0.7516 - val_loss: 1.0027 - val_acc: 0.7330\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9635 - acc: 0.7496 - val_loss: 0.9928 - val_acc: 0.7390\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9633 - acc: 0.7500 - val_loss: 0.9914 - val_acc: 0.7360\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9620 - acc: 0.7505 - val_loss: 0.9908 - val_acc: 0.7400\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9615 - acc: 0.7519 - val_loss: 0.9928 - val_acc: 0.7350\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9609 - acc: 0.7520 - val_loss: 0.9903 - val_acc: 0.7400\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9597 - acc: 0.7533 - val_loss: 0.9970 - val_acc: 0.7290\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9608 - acc: 0.7504 - val_loss: 0.9945 - val_acc: 0.7300\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9608 - acc: 0.7505 - val_loss: 0.9997 - val_acc: 0.7280\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9591 - acc: 0.7521 - val_loss: 0.9886 - val_acc: 0.7270\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9581 - acc: 0.7524 - val_loss: 0.9852 - val_acc: 0.7340\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9576 - acc: 0.7540 - val_loss: 0.9862 - val_acc: 0.7240\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9560 - acc: 0.7551 - val_loss: 0.9906 - val_acc: 0.7350\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9575 - acc: 0.7515 - val_loss: 0.9875 - val_acc: 0.7390\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9575 - acc: 0.7516 - val_loss: 0.9897 - val_acc: 0.7300\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9558 - acc: 0.7536 - val_loss: 0.9856 - val_acc: 0.7310\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9553 - acc: 0.7524 - val_loss: 0.9840 - val_acc: 0.7410\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9549 - acc: 0.7529 - val_loss: 0.9828 - val_acc: 0.7360\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9535 - acc: 0.7560 - val_loss: 0.9974 - val_acc: 0.7450\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9547 - acc: 0.7532 - val_loss: 0.9850 - val_acc: 0.7420\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9542 - acc: 0.7537 - val_loss: 0.9885 - val_acc: 0.7310\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9533 - acc: 0.7541 - val_loss: 0.9824 - val_acc: 0.7380\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9529 - acc: 0.7525 - val_loss: 0.9911 - val_acc: 0.7320\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9528 - acc: 0.7541 - val_loss: 0.9975 - val_acc: 0.7330\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9516 - acc: 0.7524 - val_loss: 1.0089 - val_acc: 0.7300\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9513 - acc: 0.7536 - val_loss: 0.9886 - val_acc: 0.7320\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9505 - acc: 0.7545 - val_loss: 1.0063 - val_acc: 0.7310\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9521 - acc: 0.7536 - val_loss: 1.0042 - val_acc: 0.7200\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9501 - acc: 0.7536 - val_loss: 0.9827 - val_acc: 0.7340\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9494 - acc: 0.7549 - val_loss: 0.9845 - val_acc: 0.7450\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9503 - acc: 0.7536 - val_loss: 0.9818 - val_acc: 0.7300\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9484 - acc: 0.7539 - val_loss: 0.9881 - val_acc: 0.7370\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9473 - acc: 0.7565 - val_loss: 0.9875 - val_acc: 0.7400\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9474 - acc: 0.7552 - val_loss: 0.9835 - val_acc: 0.7370\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9465 - acc: 0.7532 - val_loss: 1.0006 - val_acc: 0.7300\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9461 - acc: 0.7536 - val_loss: 0.9799 - val_acc: 0.7450\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9474 - acc: 0.7560 - val_loss: 0.9857 - val_acc: 0.7340\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9455 - acc: 0.7560 - val_loss: 0.9782 - val_acc: 0.7340\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9456 - acc: 0.7535 - val_loss: 0.9772 - val_acc: 0.7420\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9452 - acc: 0.7543 - val_loss: 0.9780 - val_acc: 0.7460\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9444 - acc: 0.7545 - val_loss: 0.9852 - val_acc: 0.7470\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9436 - acc: 0.7579 - val_loss: 0.9758 - val_acc: 0.7460\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9432 - acc: 0.7569 - val_loss: 0.9800 - val_acc: 0.7460\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9421 - acc: 0.7560 - val_loss: 0.9761 - val_acc: 0.7410\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9428 - acc: 0.7545 - val_loss: 0.9810 - val_acc: 0.7370\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9426 - acc: 0.7544 - val_loss: 0.9928 - val_acc: 0.7270\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9428 - acc: 0.7564 - val_loss: 0.9791 - val_acc: 0.7430\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9411 - acc: 0.7579 - val_loss: 1.0083 - val_acc: 0.7240\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9407 - acc: 0.7569 - val_loss: 0.9742 - val_acc: 0.7430\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9406 - acc: 0.7548 - val_loss: 0.9993 - val_acc: 0.7340\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9416 - acc: 0.7599 - val_loss: 0.9776 - val_acc: 0.7350\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9400 - acc: 0.7585 - val_loss: 0.9894 - val_acc: 0.7400\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9406 - acc: 0.7557 - val_loss: 0.9735 - val_acc: 0.7430\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9384 - acc: 0.7571 - val_loss: 0.9743 - val_acc: 0.7380\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9393 - acc: 0.7595 - val_loss: 0.9773 - val_acc: 0.7360\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9380 - acc: 0.7565 - val_loss: 0.9836 - val_acc: 0.7310\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9407 - acc: 0.7595 - val_loss: 0.9803 - val_acc: 0.7430\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9373 - acc: 0.7564 - val_loss: 0.9798 - val_acc: 0.7500\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9376 - acc: 0.7577 - val_loss: 0.9758 - val_acc: 0.7420\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9369 - acc: 0.7584 - val_loss: 0.9753 - val_acc: 0.7430\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9369 - acc: 0.7589 - val_loss: 0.9712 - val_acc: 0.7440\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9359 - acc: 0.7583 - val_loss: 0.9763 - val_acc: 0.7420\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9365 - acc: 0.7563 - val_loss: 0.9757 - val_acc: 0.7410\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9352 - acc: 0.7583 - val_loss: 0.9725 - val_acc: 0.7340\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9347 - acc: 0.7583 - val_loss: 0.9834 - val_acc: 0.7400\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9347 - acc: 0.7583 - val_loss: 0.9702 - val_acc: 0.7400\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9322 - acc: 0.7607 - val_loss: 1.0033 - val_acc: 0.7190\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9339 - acc: 0.7595 - val_loss: 0.9771 - val_acc: 0.7360\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9340 - acc: 0.7581 - val_loss: 0.9724 - val_acc: 0.7410\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9329 - acc: 0.7577 - val_loss: 0.9684 - val_acc: 0.7380\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9328 - acc: 0.7577 - val_loss: 0.9704 - val_acc: 0.7380\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9325 - acc: 0.7607 - val_loss: 0.9793 - val_acc: 0.7380\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9326 - acc: 0.7600 - val_loss: 0.9753 - val_acc: 0.7410\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9316 - acc: 0.7588 - val_loss: 0.9683 - val_acc: 0.7420\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9319 - acc: 0.7565 - val_loss: 0.9769 - val_acc: 0.7320\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9321 - acc: 0.7601 - val_loss: 0.9725 - val_acc: 0.7420\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9304 - acc: 0.7593 - val_loss: 0.9714 - val_acc: 0.7370\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9287 - acc: 0.7619 - val_loss: 0.9875 - val_acc: 0.7310\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9294 - acc: 0.7600 - val_loss: 0.9749 - val_acc: 0.7450\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9282 - acc: 0.7589 - val_loss: 0.9727 - val_acc: 0.7390\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9278 - acc: 0.7604 - val_loss: 0.9774 - val_acc: 0.7410\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9287 - acc: 0.7587 - val_loss: 0.9738 - val_acc: 0.7410\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9267 - acc: 0.7600 - val_loss: 0.9731 - val_acc: 0.7460\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9261 - acc: 0.7599 - val_loss: 0.9884 - val_acc: 0.7310\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9276 - acc: 0.7600 - val_loss: 0.9762 - val_acc: 0.7410\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9268 - acc: 0.7592 - val_loss: 0.9698 - val_acc: 0.7370\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9260 - acc: 0.7597 - val_loss: 0.9658 - val_acc: 0.7340\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9259 - acc: 0.7613 - val_loss: 0.9713 - val_acc: 0.7410\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9255 - acc: 0.7584 - val_loss: 0.9644 - val_acc: 0.7480\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9250 - acc: 0.7627 - val_loss: 0.9671 - val_acc: 0.7370\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9246 - acc: 0.7627 - val_loss: 0.9710 - val_acc: 0.7420\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9242 - acc: 0.7623 - val_loss: 0.9706 - val_acc: 0.7350\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9252 - acc: 0.7603 - val_loss: 0.9855 - val_acc: 0.7330\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9241 - acc: 0.7584 - val_loss: 0.9679 - val_acc: 0.7430\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9242 - acc: 0.7599 - val_loss: 0.9671 - val_acc: 0.7390\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9234 - acc: 0.7596 - val_loss: 0.9690 - val_acc: 0.7340\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9251 - acc: 0.7593 - val_loss: 0.9648 - val_acc: 0.7400\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9207 - acc: 0.7640 - val_loss: 0.9701 - val_acc: 0.7400\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9235 - acc: 0.7625 - val_loss: 0.9633 - val_acc: 0.7420\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9215 - acc: 0.7608 - val_loss: 0.9749 - val_acc: 0.7330\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9214 - acc: 0.7603 - val_loss: 0.9950 - val_acc: 0.7340\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9207 - acc: 0.7629 - val_loss: 0.9703 - val_acc: 0.7360\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9196 - acc: 0.7615 - val_loss: 0.9837 - val_acc: 0.7300\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9200 - acc: 0.7629 - val_loss: 0.9637 - val_acc: 0.7430\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9202 - acc: 0.7628 - val_loss: 0.9695 - val_acc: 0.7470\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9188 - acc: 0.7624 - val_loss: 0.9680 - val_acc: 0.7440\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9209 - acc: 0.7609 - val_loss: 1.0155 - val_acc: 0.7240\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9198 - acc: 0.7619 - val_loss: 0.9757 - val_acc: 0.7400\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9194 - acc: 0.7592 - val_loss: 0.9625 - val_acc: 0.7510\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9186 - acc: 0.7620 - val_loss: 0.9611 - val_acc: 0.7430\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9169 - acc: 0.7617 - val_loss: 0.9634 - val_acc: 0.7410\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9167 - acc: 0.7659 - val_loss: 0.9621 - val_acc: 0.7490\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9159 - acc: 0.7604 - val_loss: 0.9587 - val_acc: 0.7420\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9156 - acc: 0.7631 - val_loss: 0.9671 - val_acc: 0.7370\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9147 - acc: 0.7639 - val_loss: 0.9668 - val_acc: 0.7320\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9162 - acc: 0.7620 - val_loss: 0.9586 - val_acc: 0.7400\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9160 - acc: 0.7648 - val_loss: 0.9611 - val_acc: 0.7340\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9149 - acc: 0.7607 - val_loss: 0.9647 - val_acc: 0.7380\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9133 - acc: 0.7640 - val_loss: 0.9812 - val_acc: 0.7270\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9163 - acc: 0.7619 - val_loss: 0.9687 - val_acc: 0.7320\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9155 - acc: 0.7603 - val_loss: 0.9722 - val_acc: 0.7420\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9146 - acc: 0.7651 - val_loss: 0.9608 - val_acc: 0.7420\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9136 - acc: 0.7628 - val_loss: 0.9745 - val_acc: 0.7440\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9129 - acc: 0.7617 - val_loss: 0.9564 - val_acc: 0.7430\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9113 - acc: 0.7639 - val_loss: 0.9717 - val_acc: 0.7470\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9130 - acc: 0.7612 - val_loss: 0.9577 - val_acc: 0.7430\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9126 - acc: 0.7636 - val_loss: 0.9615 - val_acc: 0.7400\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9116 - acc: 0.7633 - val_loss: 0.9535 - val_acc: 0.7490\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9113 - acc: 0.7620 - val_loss: 0.9612 - val_acc: 0.7470\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9100 - acc: 0.7648 - val_loss: 0.9600 - val_acc: 0.7450\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9108 - acc: 0.7641 - val_loss: 0.9567 - val_acc: 0.7510\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9093 - acc: 0.7641 - val_loss: 0.9614 - val_acc: 0.7420\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9091 - acc: 0.7604 - val_loss: 0.9570 - val_acc: 0.7420\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9096 - acc: 0.7657 - val_loss: 0.9566 - val_acc: 0.7430\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9094 - acc: 0.7643 - val_loss: 0.9901 - val_acc: 0.7350\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9109 - acc: 0.7613 - val_loss: 1.0057 - val_acc: 0.7120\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9097 - acc: 0.7643 - val_loss: 0.9667 - val_acc: 0.7370\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9077 - acc: 0.7649 - val_loss: 0.9775 - val_acc: 0.7360\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9098 - acc: 0.7631 - val_loss: 0.9617 - val_acc: 0.7520\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9084 - acc: 0.7664 - val_loss: 0.9651 - val_acc: 0.7370\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9070 - acc: 0.7664 - val_loss: 0.9789 - val_acc: 0.7290\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9085 - acc: 0.7631 - val_loss: 0.9698 - val_acc: 0.7430\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9061 - acc: 0.7648 - val_loss: 0.9661 - val_acc: 0.7370\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9058 - acc: 0.7681 - val_loss: 0.9603 - val_acc: 0.7420\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9049 - acc: 0.7676 - val_loss: 0.9534 - val_acc: 0.7360\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9055 - acc: 0.7641 - val_loss: 0.9656 - val_acc: 0.7480\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9079 - acc: 0.7647 - val_loss: 0.9549 - val_acc: 0.7440\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9045 - acc: 0.7635 - val_loss: 0.9528 - val_acc: 0.7460\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9055 - acc: 0.7643 - val_loss: 0.9507 - val_acc: 0.7450\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9050 - acc: 0.7684 - val_loss: 0.9598 - val_acc: 0.7490\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9040 - acc: 0.7661 - val_loss: 0.9773 - val_acc: 0.7240\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9059 - acc: 0.7636 - val_loss: 0.9603 - val_acc: 0.7420\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9033 - acc: 0.7675 - val_loss: 0.9549 - val_acc: 0.7480\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9026 - acc: 0.7640 - val_loss: 0.9518 - val_acc: 0.7430\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9041 - acc: 0.7667 - val_loss: 0.9581 - val_acc: 0.7300\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9032 - acc: 0.7647 - val_loss: 0.9501 - val_acc: 0.7440\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9023 - acc: 0.7667 - val_loss: 0.9488 - val_acc: 0.7470\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9010 - acc: 0.7676 - val_loss: 0.9686 - val_acc: 0.7350\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9023 - acc: 0.7663 - val_loss: 0.9532 - val_acc: 0.7510\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9015 - acc: 0.7672 - val_loss: 0.9480 - val_acc: 0.7450\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9007 - acc: 0.7656 - val_loss: 0.9569 - val_acc: 0.7410\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9006 - acc: 0.7636 - val_loss: 0.9579 - val_acc: 0.7350\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9003 - acc: 0.7677 - val_loss: 0.9483 - val_acc: 0.7480\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8991 - acc: 0.7672 - val_loss: 0.9490 - val_acc: 0.7430\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8995 - acc: 0.7681 - val_loss: 0.9493 - val_acc: 0.7380\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8990 - acc: 0.7667 - val_loss: 0.9503 - val_acc: 0.7400\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8986 - acc: 0.7685 - val_loss: 0.9504 - val_acc: 0.7460\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8989 - acc: 0.7683 - val_loss: 0.9673 - val_acc: 0.7390\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8993 - acc: 0.7667 - val_loss: 0.9504 - val_acc: 0.7500\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8964 - acc: 0.7673 - val_loss: 0.9580 - val_acc: 0.7410\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8998 - acc: 0.7657 - val_loss: 0.9974 - val_acc: 0.7150\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8985 - acc: 0.7652 - val_loss: 0.9581 - val_acc: 0.7350\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8981 - acc: 0.7685 - val_loss: 0.9503 - val_acc: 0.7470\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8972 - acc: 0.7672 - val_loss: 0.9463 - val_acc: 0.7490\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8970 - acc: 0.7661 - val_loss: 0.9687 - val_acc: 0.7470\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8981 - acc: 0.7684 - val_loss: 0.9459 - val_acc: 0.7510\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8992 - acc: 0.7680 - val_loss: 0.9523 - val_acc: 0.7490\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8965 - acc: 0.7675 - val_loss: 0.9492 - val_acc: 0.7410\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8979 - acc: 0.7673 - val_loss: 0.9477 - val_acc: 0.7470\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8951 - acc: 0.7693 - val_loss: 0.9616 - val_acc: 0.7500\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8964 - acc: 0.7673 - val_loss: 0.9523 - val_acc: 0.7520\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8956 - acc: 0.7685 - val_loss: 0.9517 - val_acc: 0.7530\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8963 - acc: 0.7669 - val_loss: 0.9543 - val_acc: 0.7300\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8936 - acc: 0.7680 - val_loss: 0.9548 - val_acc: 0.7400\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8958 - acc: 0.7677 - val_loss: 0.9613 - val_acc: 0.7490\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8943 - acc: 0.7687 - val_loss: 0.9542 - val_acc: 0.7510\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8941 - acc: 0.7696 - val_loss: 0.9953 - val_acc: 0.7390\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8957 - acc: 0.7668 - val_loss: 0.9461 - val_acc: 0.7480\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8928 - acc: 0.7681 - val_loss: 0.9500 - val_acc: 0.7360\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8929 - acc: 0.7711 - val_loss: 0.9464 - val_acc: 0.7540\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8919 - acc: 0.7693 - val_loss: 0.9448 - val_acc: 0.7520\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8940 - acc: 0.7688 - val_loss: 0.9433 - val_acc: 0.7480\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8915 - acc: 0.7681 - val_loss: 0.9468 - val_acc: 0.7510\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8929 - acc: 0.7688 - val_loss: 0.9696 - val_acc: 0.7420\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8921 - acc: 0.7699 - val_loss: 0.9515 - val_acc: 0.7440\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8911 - acc: 0.7679 - val_loss: 0.9475 - val_acc: 0.7380\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8913 - acc: 0.7659 - val_loss: 0.9575 - val_acc: 0.7410\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8921 - acc: 0.7689 - val_loss: 0.9440 - val_acc: 0.7510\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8889 - acc: 0.7688 - val_loss: 0.9650 - val_acc: 0.7320\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8934 - acc: 0.7671 - val_loss: 0.9646 - val_acc: 0.7310\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8891 - acc: 0.7700 - val_loss: 0.9461 - val_acc: 0.7460\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8892 - acc: 0.7693 - val_loss: 0.9408 - val_acc: 0.7480\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8926 - acc: 0.7712 - val_loss: 0.9841 - val_acc: 0.7220\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8917 - acc: 0.7668 - val_loss: 0.9440 - val_acc: 0.7510\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8898 - acc: 0.7696 - val_loss: 0.9420 - val_acc: 0.7480\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8890 - acc: 0.7715 - val_loss: 0.9496 - val_acc: 0.7510\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8887 - acc: 0.7692 - val_loss: 0.9442 - val_acc: 0.7490\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8879 - acc: 0.7683 - val_loss: 0.9563 - val_acc: 0.7360\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8893 - acc: 0.7676 - val_loss: 0.9451 - val_acc: 0.7460\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8891 - acc: 0.7675 - val_loss: 0.9446 - val_acc: 0.7400\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8862 - acc: 0.7716 - val_loss: 0.9689 - val_acc: 0.7440\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8882 - acc: 0.7689 - val_loss: 0.9389 - val_acc: 0.7460\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8893 - acc: 0.7696 - val_loss: 0.9677 - val_acc: 0.7320\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8880 - acc: 0.7691 - val_loss: 0.9525 - val_acc: 0.7530\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8869 - acc: 0.7669 - val_loss: 0.9473 - val_acc: 0.7480\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8847 - acc: 0.7695 - val_loss: 0.9766 - val_acc: 0.7210\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8880 - acc: 0.7677 - val_loss: 0.9566 - val_acc: 0.7470\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8857 - acc: 0.7713 - val_loss: 0.9496 - val_acc: 0.7520\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8868 - acc: 0.7716 - val_loss: 0.9757 - val_acc: 0.7210\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8865 - acc: 0.7695 - val_loss: 0.9425 - val_acc: 0.7500\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8837 - acc: 0.7715 - val_loss: 0.9537 - val_acc: 0.7420\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8844 - acc: 0.7701 - val_loss: 0.9486 - val_acc: 0.7420\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8861 - acc: 0.7688 - val_loss: 0.9431 - val_acc: 0.7560\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8858 - acc: 0.7716 - val_loss: 0.9585 - val_acc: 0.7360\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8846 - acc: 0.7700 - val_loss: 0.9400 - val_acc: 0.7490\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8839 - acc: 0.7720 - val_loss: 0.9420 - val_acc: 0.7430\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8888 - acc: 0.7688 - val_loss: 0.9536 - val_acc: 0.7430\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8852 - acc: 0.7681 - val_loss: 0.9430 - val_acc: 0.7490\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8833 - acc: 0.7696 - val_loss: 0.9391 - val_acc: 0.7490\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8837 - acc: 0.7689 - val_loss: 0.9537 - val_acc: 0.7330\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8817 - acc: 0.7700 - val_loss: 0.9430 - val_acc: 0.7440\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8819 - acc: 0.7724 - val_loss: 0.9407 - val_acc: 0.7470\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8816 - acc: 0.7685 - val_loss: 0.9373 - val_acc: 0.7460\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8814 - acc: 0.7744 - val_loss: 0.9386 - val_acc: 0.7540\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8809 - acc: 0.7739 - val_loss: 0.9375 - val_acc: 0.7540\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8820 - acc: 0.7675 - val_loss: 0.9405 - val_acc: 0.7480\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8805 - acc: 0.7705 - val_loss: 0.9444 - val_acc: 0.7410\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8842 - acc: 0.7688 - val_loss: 0.9565 - val_acc: 0.7380\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8797 - acc: 0.7716 - val_loss: 0.9436 - val_acc: 0.7390\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8809 - acc: 0.7715 - val_loss: 0.9477 - val_acc: 0.7490\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8813 - acc: 0.7729 - val_loss: 0.9418 - val_acc: 0.7500\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8810 - acc: 0.7717 - val_loss: 0.9353 - val_acc: 0.7530\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8790 - acc: 0.7729 - val_loss: 0.9475 - val_acc: 0.7530\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8789 - acc: 0.7725 - val_loss: 0.9390 - val_acc: 0.7480\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8788 - acc: 0.7720 - val_loss: 0.9406 - val_acc: 0.7490\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8807 - acc: 0.7707 - val_loss: 0.9737 - val_acc: 0.7200\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8805 - acc: 0.7719 - val_loss: 0.9528 - val_acc: 0.7470\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8830 - acc: 0.7705 - val_loss: 0.9406 - val_acc: 0.7400\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8791 - acc: 0.7723 - val_loss: 0.9320 - val_acc: 0.7570\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8779 - acc: 0.7720 - val_loss: 0.9650 - val_acc: 0.7290\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8778 - acc: 0.7707 - val_loss: 0.9562 - val_acc: 0.7420\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8804 - acc: 0.7680 - val_loss: 0.9350 - val_acc: 0.7530\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8770 - acc: 0.7728 - val_loss: 0.9478 - val_acc: 0.7420\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8799 - acc: 0.7693 - val_loss: 0.9307 - val_acc: 0.7480\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8779 - acc: 0.7727 - val_loss: 0.9591 - val_acc: 0.7340\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8767 - acc: 0.7735 - val_loss: 0.9560 - val_acc: 0.7320\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8782 - acc: 0.7693 - val_loss: 0.9393 - val_acc: 0.7500\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8764 - acc: 0.7705 - val_loss: 0.9384 - val_acc: 0.7520\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8757 - acc: 0.7735 - val_loss: 0.9513 - val_acc: 0.7430\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8794 - acc: 0.7708 - val_loss: 0.9366 - val_acc: 0.7430\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8782 - acc: 0.7691 - val_loss: 0.9325 - val_acc: 0.7440\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8735 - acc: 0.7745 - val_loss: 0.9308 - val_acc: 0.7470\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8750 - acc: 0.7707 - val_loss: 0.9447 - val_acc: 0.7450\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8760 - acc: 0.7719 - val_loss: 0.9425 - val_acc: 0.7510\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8783 - acc: 0.7727 - val_loss: 0.9502 - val_acc: 0.7440\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8754 - acc: 0.7725 - val_loss: 0.9368 - val_acc: 0.7450\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8737 - acc: 0.7716 - val_loss: 0.9353 - val_acc: 0.7480\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8736 - acc: 0.7735 - val_loss: 0.9419 - val_acc: 0.7470\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8744 - acc: 0.7747 - val_loss: 0.9455 - val_acc: 0.7480\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8726 - acc: 0.7745 - val_loss: 0.9312 - val_acc: 0.7430\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8730 - acc: 0.7749 - val_loss: 0.9362 - val_acc: 0.7480\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8727 - acc: 0.7757 - val_loss: 0.9334 - val_acc: 0.7550\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8700 - acc: 0.7767 - val_loss: 0.9355 - val_acc: 0.7480\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8695 - acc: 0.7761 - val_loss: 0.9504 - val_acc: 0.7400\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8744 - acc: 0.7736 - val_loss: 0.9294 - val_acc: 0.7490\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8727 - acc: 0.7737 - val_loss: 0.9742 - val_acc: 0.7450\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8715 - acc: 0.7737 - val_loss: 0.9349 - val_acc: 0.7510\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8746 - acc: 0.7728 - val_loss: 0.9446 - val_acc: 0.7550\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8742 - acc: 0.7728 - val_loss: 0.9283 - val_acc: 0.7480\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8721 - acc: 0.7739 - val_loss: 0.9309 - val_acc: 0.7550\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8704 - acc: 0.7757 - val_loss: 0.9262 - val_acc: 0.7500\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8675 - acc: 0.7751 - val_loss: 0.9273 - val_acc: 0.7460\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8686 - acc: 0.7748 - val_loss: 0.9255 - val_acc: 0.7500\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8710 - acc: 0.7713 - val_loss: 0.9421 - val_acc: 0.7380\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8731 - acc: 0.7719 - val_loss: 0.9327 - val_acc: 0.7480\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8696 - acc: 0.7736 - val_loss: 1.0409 - val_acc: 0.7090\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8772 - acc: 0.7711 - val_loss: 0.9463 - val_acc: 0.7500\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8710 - acc: 0.7731 - val_loss: 0.9323 - val_acc: 0.7510\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8690 - acc: 0.7751 - val_loss: 0.9421 - val_acc: 0.7440\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8680 - acc: 0.7759 - val_loss: 0.9414 - val_acc: 0.7400\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8687 - acc: 0.7739 - val_loss: 0.9252 - val_acc: 0.7520\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8690 - acc: 0.7752 - val_loss: 0.9291 - val_acc: 0.7450\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8686 - acc: 0.7764 - val_loss: 0.9920 - val_acc: 0.7390\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8704 - acc: 0.7723 - val_loss: 0.9497 - val_acc: 0.7360\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8703 - acc: 0.7736 - val_loss: 0.9270 - val_acc: 0.7560\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8723 - acc: 0.7732 - val_loss: 0.9347 - val_acc: 0.7520\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8661 - acc: 0.7740 - val_loss: 0.9429 - val_acc: 0.7460\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8685 - acc: 0.7744 - val_loss: 0.9221 - val_acc: 0.7570\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8646 - acc: 0.7769 - val_loss: 0.9303 - val_acc: 0.7530\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8681 - acc: 0.7739 - val_loss: 0.9289 - val_acc: 0.7520\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8662 - acc: 0.7773 - val_loss: 0.9703 - val_acc: 0.7250\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8656 - acc: 0.7737 - val_loss: 0.9286 - val_acc: 0.7520\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8677 - acc: 0.7747 - val_loss: 0.9257 - val_acc: 0.7440\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8656 - acc: 0.7732 - val_loss: 0.9308 - val_acc: 0.7440\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8654 - acc: 0.7748 - val_loss: 0.9208 - val_acc: 0.7500\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8643 - acc: 0.7744 - val_loss: 0.9468 - val_acc: 0.7360\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8685 - acc: 0.7749 - val_loss: 0.9280 - val_acc: 0.7510\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8647 - acc: 0.7749 - val_loss: 0.9328 - val_acc: 0.7450\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8633 - acc: 0.7780 - val_loss: 0.9257 - val_acc: 0.7500\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8628 - acc: 0.7747 - val_loss: 0.9312 - val_acc: 0.7470\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8645 - acc: 0.7773 - val_loss: 0.9251 - val_acc: 0.7500\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8621 - acc: 0.7788 - val_loss: 0.9515 - val_acc: 0.7360\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8636 - acc: 0.7739 - val_loss: 0.9339 - val_acc: 0.7510\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8666 - acc: 0.7728 - val_loss: 0.9239 - val_acc: 0.7540\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8627 - acc: 0.7776 - val_loss: 0.9335 - val_acc: 0.7420\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8622 - acc: 0.7788 - val_loss: 0.9413 - val_acc: 0.7400\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8640 - acc: 0.7767 - val_loss: 0.9330 - val_acc: 0.7500\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8614 - acc: 0.7764 - val_loss: 0.9183 - val_acc: 0.7540\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8649 - acc: 0.7765 - val_loss: 0.9453 - val_acc: 0.7370\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8648 - acc: 0.7736 - val_loss: 0.9453 - val_acc: 0.7350\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8627 - acc: 0.7780 - val_loss: 0.9521 - val_acc: 0.7330\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8656 - acc: 0.7759 - val_loss: 0.9285 - val_acc: 0.7450\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8594 - acc: 0.7783 - val_loss: 0.9519 - val_acc: 0.7330\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8640 - acc: 0.7731 - val_loss: 0.9172 - val_acc: 0.7520\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8615 - acc: 0.7753 - val_loss: 0.9334 - val_acc: 0.7530\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8610 - acc: 0.7772 - val_loss: 0.9287 - val_acc: 0.7510\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8594 - acc: 0.7767 - val_loss: 0.9215 - val_acc: 0.7510\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8622 - acc: 0.7745 - val_loss: 0.9374 - val_acc: 0.7510\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8600 - acc: 0.7776 - val_loss: 0.9359 - val_acc: 0.7450\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8595 - acc: 0.7757 - val_loss: 0.9635 - val_acc: 0.7300\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8621 - acc: 0.7748 - val_loss: 0.9653 - val_acc: 0.7380\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8636 - acc: 0.7797 - val_loss: 0.9433 - val_acc: 0.7330\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8580 - acc: 0.7787 - val_loss: 0.9362 - val_acc: 0.7460\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8615 - acc: 0.7783 - val_loss: 0.9224 - val_acc: 0.7550\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8572 - acc: 0.7781 - val_loss: 0.9179 - val_acc: 0.7520\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8587 - acc: 0.7757 - val_loss: 0.9400 - val_acc: 0.7520\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8600 - acc: 0.7787 - val_loss: 0.9218 - val_acc: 0.7550\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8630 - acc: 0.7784 - val_loss: 0.9249 - val_acc: 0.7470\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8582 - acc: 0.7776 - val_loss: 0.9296 - val_acc: 0.7410\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8592 - acc: 0.7779 - val_loss: 0.9202 - val_acc: 0.7530\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8598 - acc: 0.7783 - val_loss: 0.9327 - val_acc: 0.7390\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8596 - acc: 0.7777 - val_loss: 0.9183 - val_acc: 0.7480\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8585 - acc: 0.7769 - val_loss: 0.9391 - val_acc: 0.7380\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8603 - acc: 0.7752 - val_loss: 0.9157 - val_acc: 0.7520\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8595 - acc: 0.7775 - val_loss: 0.9546 - val_acc: 0.7400\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8561 - acc: 0.7763 - val_loss: 0.9416 - val_acc: 0.7540\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8574 - acc: 0.7789 - val_loss: 0.9429 - val_acc: 0.7380\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8637 - acc: 0.7759 - val_loss: 0.9247 - val_acc: 0.7510\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8567 - acc: 0.7773 - val_loss: 0.9223 - val_acc: 0.7480\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8566 - acc: 0.7791 - val_loss: 0.9235 - val_acc: 0.7550\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8540 - acc: 0.7804 - val_loss: 0.9775 - val_acc: 0.7390\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8620 - acc: 0.7771 - val_loss: 0.9245 - val_acc: 0.7490\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8594 - acc: 0.7769 - val_loss: 0.9298 - val_acc: 0.7490\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8558 - acc: 0.7805 - val_loss: 0.9275 - val_acc: 0.7540\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8536 - acc: 0.7793 - val_loss: 0.9275 - val_acc: 0.7490\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8598 - acc: 0.7740 - val_loss: 0.9306 - val_acc: 0.7440\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8561 - acc: 0.7783 - val_loss: 0.9256 - val_acc: 0.7430\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8547 - acc: 0.7795 - val_loss: 0.9208 - val_acc: 0.7490\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8537 - acc: 0.7792 - val_loss: 0.9466 - val_acc: 0.7370\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8543 - acc: 0.7795 - val_loss: 0.9176 - val_acc: 0.7470\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8567 - acc: 0.7795 - val_loss: 0.9261 - val_acc: 0.7420\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8546 - acc: 0.7788 - val_loss: 0.9535 - val_acc: 0.7530\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8564 - acc: 0.7796 - val_loss: 0.9150 - val_acc: 0.7570\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8554 - acc: 0.7791 - val_loss: 0.9360 - val_acc: 0.7420\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8544 - acc: 0.7777 - val_loss: 0.9262 - val_acc: 0.7480\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8557 - acc: 0.7783 - val_loss: 0.9833 - val_acc: 0.7260\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8601 - acc: 0.7757 - val_loss: 1.0039 - val_acc: 0.7080\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8588 - acc: 0.7747 - val_loss: 0.9275 - val_acc: 0.7470\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8541 - acc: 0.7789 - val_loss: 0.9331 - val_acc: 0.7450\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8523 - acc: 0.7805 - val_loss: 0.9570 - val_acc: 0.7450\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8521 - acc: 0.7796 - val_loss: 0.9663 - val_acc: 0.7410\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8622 - acc: 0.7761 - val_loss: 0.9627 - val_acc: 0.7470\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8567 - acc: 0.7795 - val_loss: 0.9238 - val_acc: 0.7460\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8551 - acc: 0.7789 - val_loss: 0.9210 - val_acc: 0.7500\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8553 - acc: 0.7743 - val_loss: 0.9422 - val_acc: 0.7300\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8512 - acc: 0.7784 - val_loss: 0.9221 - val_acc: 0.7560\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8548 - acc: 0.7776 - val_loss: 0.9261 - val_acc: 0.7510\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8547 - acc: 0.7792 - val_loss: 0.9098 - val_acc: 0.7510\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8511 - acc: 0.7789 - val_loss: 0.9184 - val_acc: 0.7560\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8553 - acc: 0.7775 - val_loss: 0.9382 - val_acc: 0.7410\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8504 - acc: 0.7799 - val_loss: 0.9276 - val_acc: 0.7470\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8518 - acc: 0.7819 - val_loss: 0.9181 - val_acc: 0.7520\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8541 - acc: 0.7784 - val_loss: 0.9134 - val_acc: 0.7480\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8481 - acc: 0.7804 - val_loss: 0.9249 - val_acc: 0.7470\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8520 - acc: 0.7827 - val_loss: 0.9115 - val_acc: 0.7530\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8494 - acc: 0.7805 - val_loss: 0.9174 - val_acc: 0.7480\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8557 - acc: 0.7793 - val_loss: 0.9208 - val_acc: 0.7480\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8492 - acc: 0.7817 - val_loss: 0.9199 - val_acc: 0.7510\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8526 - acc: 0.7779 - val_loss: 0.9256 - val_acc: 0.7430\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8490 - acc: 0.7791 - val_loss: 0.9738 - val_acc: 0.7270\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8514 - acc: 0.7803 - val_loss: 0.9231 - val_acc: 0.7490\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8513 - acc: 0.7785 - val_loss: 0.9257 - val_acc: 0.7450\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8517 - acc: 0.7795 - val_loss: 0.9225 - val_acc: 0.7520\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8484 - acc: 0.7812 - val_loss: 0.9321 - val_acc: 0.7530\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8531 - acc: 0.7772 - val_loss: 0.9166 - val_acc: 0.7490\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8547 - acc: 0.7749 - val_loss: 0.9374 - val_acc: 0.7450\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8536 - acc: 0.7799 - val_loss: 0.9329 - val_acc: 0.7400\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8492 - acc: 0.7808 - val_loss: 0.9249 - val_acc: 0.7400\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8494 - acc: 0.7799 - val_loss: 0.9180 - val_acc: 0.7540\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8511 - acc: 0.7808 - val_loss: 0.9187 - val_acc: 0.7490\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8455 - acc: 0.7844 - val_loss: 0.9120 - val_acc: 0.7510\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8489 - acc: 0.7825 - val_loss: 0.9144 - val_acc: 0.7490\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8532 - acc: 0.7804 - val_loss: 0.9142 - val_acc: 0.7420\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8490 - acc: 0.7831 - val_loss: 0.9380 - val_acc: 0.7410\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8463 - acc: 0.7825 - val_loss: 0.9080 - val_acc: 0.7530\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8492 - acc: 0.7813 - val_loss: 0.9286 - val_acc: 0.7390\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8451 - acc: 0.7839 - val_loss: 0.9104 - val_acc: 0.7570\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8456 - acc: 0.7797 - val_loss: 0.9129 - val_acc: 0.7470\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8506 - acc: 0.7789 - val_loss: 0.9098 - val_acc: 0.7510\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8495 - acc: 0.7804 - val_loss: 0.9281 - val_acc: 0.7520\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8454 - acc: 0.7789 - val_loss: 0.9466 - val_acc: 0.7430\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8468 - acc: 0.7816 - val_loss: 0.9117 - val_acc: 0.7550\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8453 - acc: 0.7848 - val_loss: 0.9948 - val_acc: 0.7280\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8555 - acc: 0.7792 - val_loss: 0.9277 - val_acc: 0.7400\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8458 - acc: 0.7825 - val_loss: 1.1463 - val_acc: 0.6880\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8696 - acc: 0.7727 - val_loss: 0.9393 - val_acc: 0.7310\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8578 - acc: 0.7744 - val_loss: 0.9327 - val_acc: 0.7360\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8490 - acc: 0.7809 - val_loss: 0.9093 - val_acc: 0.7500\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8467 - acc: 0.7795 - val_loss: 0.9280 - val_acc: 0.7430\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8456 - acc: 0.7811 - val_loss: 0.9125 - val_acc: 0.7540\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8441 - acc: 0.7828 - val_loss: 0.9094 - val_acc: 0.7540\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8441 - acc: 0.7832 - val_loss: 0.9246 - val_acc: 0.7540\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8473 - acc: 0.7816 - val_loss: 0.9109 - val_acc: 0.7520\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8463 - acc: 0.7808 - val_loss: 0.9361 - val_acc: 0.7520\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8470 - acc: 0.7820 - val_loss: 0.9099 - val_acc: 0.7510\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8430 - acc: 0.7796 - val_loss: 0.9420 - val_acc: 0.7340\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8466 - acc: 0.7807 - val_loss: 0.9201 - val_acc: 0.7520\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8453 - acc: 0.7824 - val_loss: 0.9158 - val_acc: 0.7520\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8526 - acc: 0.7784 - val_loss: 0.9239 - val_acc: 0.7430\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8536 - acc: 0.7779 - val_loss: 0.9127 - val_acc: 0.7560\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8422 - acc: 0.7828 - val_loss: 0.9426 - val_acc: 0.7370\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8479 - acc: 0.7791 - val_loss: 0.9318 - val_acc: 0.7390\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8422 - acc: 0.7828 - val_loss: 0.9280 - val_acc: 0.7410\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8429 - acc: 0.7837 - val_loss: 0.9291 - val_acc: 0.7500\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8428 - acc: 0.7843 - val_loss: 0.9123 - val_acc: 0.7500\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8427 - acc: 0.7815 - val_loss: 0.9121 - val_acc: 0.7540\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8416 - acc: 0.7824 - val_loss: 0.9113 - val_acc: 0.7560\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8614 - acc: 0.7724 - val_loss: 0.9289 - val_acc: 0.7530\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8420 - acc: 0.7833 - val_loss: 0.9138 - val_acc: 0.7520\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8450 - acc: 0.7799 - val_loss: 0.9131 - val_acc: 0.7600\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8417 - acc: 0.7827 - val_loss: 0.9195 - val_acc: 0.7460\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8511 - acc: 0.7757 - val_loss: 0.9122 - val_acc: 0.7460\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8508 - acc: 0.7813 - val_loss: 0.9079 - val_acc: 0.7570\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8397 - acc: 0.7828 - val_loss: 0.9406 - val_acc: 0.7410\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8429 - acc: 0.7812 - val_loss: 0.9097 - val_acc: 0.7480\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8416 - acc: 0.7827 - val_loss: 0.9147 - val_acc: 0.7590\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8408 - acc: 0.7851 - val_loss: 0.9081 - val_acc: 0.7450\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8433 - acc: 0.7839 - val_loss: 0.9099 - val_acc: 0.7550\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.7820 - val_loss: 0.9129 - val_acc: 0.7500\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8385 - acc: 0.7819 - val_loss: 0.9068 - val_acc: 0.7460\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8399 - acc: 0.7819 - val_loss: 0.9146 - val_acc: 0.7460\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8456 - acc: 0.7799 - val_loss: 0.9366 - val_acc: 0.7330\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8441 - acc: 0.7837 - val_loss: 0.9153 - val_acc: 0.7450\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8402 - acc: 0.7827 - val_loss: 0.9109 - val_acc: 0.7540\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8404 - acc: 0.7828 - val_loss: 0.9463 - val_acc: 0.7470\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8442 - acc: 0.7831 - val_loss: 0.9089 - val_acc: 0.7550\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8379 - acc: 0.7825 - val_loss: 0.9208 - val_acc: 0.7510\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8387 - acc: 0.7837 - val_loss: 0.9251 - val_acc: 0.7380\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8367 - acc: 0.7841 - val_loss: 0.9551 - val_acc: 0.7480\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8407 - acc: 0.7832 - val_loss: 0.9076 - val_acc: 0.7530\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8370 - acc: 0.7847 - val_loss: 0.9339 - val_acc: 0.7460\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8391 - acc: 0.7847 - val_loss: 0.9257 - val_acc: 0.7410\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8371 - acc: 0.7840 - val_loss: 0.9476 - val_acc: 0.7280\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8413 - acc: 0.7825 - val_loss: 0.9863 - val_acc: 0.7170\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8418 - acc: 0.7845 - val_loss: 0.9236 - val_acc: 0.7520\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8455 - acc: 0.7815 - val_loss: 0.9234 - val_acc: 0.7450\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8404 - acc: 0.7833 - val_loss: 0.9170 - val_acc: 0.7470\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8381 - acc: 0.7837 - val_loss: 0.9042 - val_acc: 0.7500\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8440 - acc: 0.7804 - val_loss: 0.9408 - val_acc: 0.7430\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8387 - acc: 0.7848 - val_loss: 0.9084 - val_acc: 0.7570\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8399 - acc: 0.7823 - val_loss: 0.9256 - val_acc: 0.7410\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8369 - acc: 0.7841 - val_loss: 0.9109 - val_acc: 0.7480\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8345 - acc: 0.7839 - val_loss: 0.9348 - val_acc: 0.7390\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8377 - acc: 0.7815 - val_loss: 0.9263 - val_acc: 0.7390\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8406 - acc: 0.7827 - val_loss: 0.9300 - val_acc: 0.7400\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8389 - acc: 0.7840 - val_loss: 0.9119 - val_acc: 0.7470\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8343 - acc: 0.7849 - val_loss: 0.9154 - val_acc: 0.7470\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8428 - acc: 0.7821 - val_loss: 0.9448 - val_acc: 0.7470\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8397 - acc: 0.7823 - val_loss: 0.9216 - val_acc: 0.7460\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8383 - acc: 0.7827 - val_loss: 0.9287 - val_acc: 0.7510\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8357 - acc: 0.7841 - val_loss: 0.9332 - val_acc: 0.7380\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8453 - acc: 0.7791 - val_loss: 0.9317 - val_acc: 0.7430\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8439 - acc: 0.7823 - val_loss: 0.9097 - val_acc: 0.7490\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8392 - acc: 0.7823 - val_loss: 0.9245 - val_acc: 0.7440\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8386 - acc: 0.7863 - val_loss: 0.9463 - val_acc: 0.7320\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.7813 - val_loss: 0.9150 - val_acc: 0.7460\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8348 - acc: 0.7829 - val_loss: 0.9268 - val_acc: 0.7480\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8430 - acc: 0.7812 - val_loss: 0.9089 - val_acc: 0.7530\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8397 - acc: 0.7823 - val_loss: 0.9468 - val_acc: 0.7290\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8430 - acc: 0.7795 - val_loss: 0.9389 - val_acc: 0.7450\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8355 - acc: 0.7839 - val_loss: 0.9155 - val_acc: 0.7480\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8379 - acc: 0.7841 - val_loss: 0.9093 - val_acc: 0.7530\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8343 - acc: 0.7855 - val_loss: 0.9639 - val_acc: 0.7210\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8437 - acc: 0.7805 - val_loss: 0.9064 - val_acc: 0.7570\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8324 - acc: 0.7833 - val_loss: 0.9143 - val_acc: 0.7420\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8352 - acc: 0.7833 - val_loss: 0.9234 - val_acc: 0.7410\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8392 - acc: 0.7812 - val_loss: 1.0218 - val_acc: 0.7270\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8456 - acc: 0.7796 - val_loss: 0.9365 - val_acc: 0.7450\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8372 - acc: 0.7855 - val_loss: 0.9015 - val_acc: 0.7560\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8362 - acc: 0.7825 - val_loss: 0.9245 - val_acc: 0.7480\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8348 - acc: 0.7844 - val_loss: 0.9225 - val_acc: 0.7480\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8356 - acc: 0.7831 - val_loss: 0.9128 - val_acc: 0.7410\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8326 - acc: 0.7836 - val_loss: 0.9494 - val_acc: 0.7330\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8337 - acc: 0.7839 - val_loss: 0.9470 - val_acc: 0.7430\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.7785 - val_loss: 0.9044 - val_acc: 0.7520\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8349 - acc: 0.7841 - val_loss: 0.9299 - val_acc: 0.7350\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8371 - acc: 0.7819 - val_loss: 0.9059 - val_acc: 0.7490\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8350 - acc: 0.7832 - val_loss: 0.9075 - val_acc: 0.7550\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8300 - acc: 0.7840 - val_loss: 0.9283 - val_acc: 0.7500\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8360 - acc: 0.7843 - val_loss: 0.9371 - val_acc: 0.7320\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8332 - acc: 0.7837 - val_loss: 0.9867 - val_acc: 0.7200\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8395 - acc: 0.7855 - val_loss: 0.9353 - val_acc: 0.7460\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8366 - acc: 0.7851 - val_loss: 0.9200 - val_acc: 0.7550\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8371 - acc: 0.7836 - val_loss: 0.9088 - val_acc: 0.7460\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8326 - acc: 0.7852 - val_loss: 0.9389 - val_acc: 0.7370\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8344 - acc: 0.7837 - val_loss: 0.9233 - val_acc: 0.7440\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8401 - acc: 0.7795 - val_loss: 0.9148 - val_acc: 0.7460\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8327 - acc: 0.7856 - val_loss: 0.9245 - val_acc: 0.7460\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8334 - acc: 0.7841 - val_loss: 0.9432 - val_acc: 0.7450\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8317 - acc: 0.7863 - val_loss: 0.9539 - val_acc: 0.7410\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8355 - acc: 0.7821 - val_loss: 0.9247 - val_acc: 0.7490\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8343 - acc: 0.7836 - val_loss: 0.9096 - val_acc: 0.7510\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8347 - acc: 0.7885 - val_loss: 0.9044 - val_acc: 0.7510\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8299 - acc: 0.7853 - val_loss: 0.9157 - val_acc: 0.7530\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8376 - acc: 0.7813 - val_loss: 0.9076 - val_acc: 0.7490\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8277 - acc: 0.7883 - val_loss: 0.9228 - val_acc: 0.7390\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8286 - acc: 0.7861 - val_loss: 0.9231 - val_acc: 0.7560\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8292 - acc: 0.7891 - val_loss: 0.9084 - val_acc: 0.7490\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8294 - acc: 0.7868 - val_loss: 0.9258 - val_acc: 0.7450\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8306 - acc: 0.7885 - val_loss: 0.9304 - val_acc: 0.7450\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8377 - acc: 0.7852 - val_loss: 0.9087 - val_acc: 0.7470\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8285 - acc: 0.7865 - val_loss: 0.9284 - val_acc: 0.7480\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8414 - acc: 0.7785 - val_loss: 0.9238 - val_acc: 0.7460\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8272 - acc: 0.7875 - val_loss: 0.9104 - val_acc: 0.7510\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8316 - acc: 0.7821 - val_loss: 0.9093 - val_acc: 0.7420\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8313 - acc: 0.7860 - val_loss: 0.9056 - val_acc: 0.7530\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8433 - acc: 0.7791 - val_loss: 0.9220 - val_acc: 0.7540\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8302 - acc: 0.7885 - val_loss: 0.9435 - val_acc: 0.7500\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8335 - acc: 0.7844 - val_loss: 0.9036 - val_acc: 0.7490\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8283 - acc: 0.7880 - val_loss: 0.9634 - val_acc: 0.7270\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8262 - acc: 0.7891 - val_loss: 0.9505 - val_acc: 0.7370\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8396 - acc: 0.7813 - val_loss: 1.0088 - val_acc: 0.7130\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8302 - acc: 0.7872 - val_loss: 0.9055 - val_acc: 0.7500\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8328 - acc: 0.7852 - val_loss: 0.9195 - val_acc: 0.7490\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8313 - acc: 0.7847 - val_loss: 0.9030 - val_acc: 0.7590\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8241 - acc: 0.7875 - val_loss: 0.9228 - val_acc: 0.7430\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8260 - acc: 0.7884 - val_loss: 0.9140 - val_acc: 0.7390\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8250 - acc: 0.7897 - val_loss: 0.9159 - val_acc: 0.7450\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8265 - acc: 0.7857 - val_loss: 0.9432 - val_acc: 0.7360\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8285 - acc: 0.7853 - val_loss: 0.9019 - val_acc: 0.7560\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8272 - acc: 0.7875 - val_loss: 0.9084 - val_acc: 0.7480\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8332 - acc: 0.7852 - val_loss: 0.9132 - val_acc: 0.7530\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8282 - acc: 0.7853 - val_loss: 0.9788 - val_acc: 0.7170\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8358 - acc: 0.7828 - val_loss: 0.9195 - val_acc: 0.7560\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8296 - acc: 0.7859 - val_loss: 0.9349 - val_acc: 0.7410\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8282 - acc: 0.7863 - val_loss: 0.9081 - val_acc: 0.7470\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8273 - acc: 0.7904 - val_loss: 0.9290 - val_acc: 0.7400\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8311 - acc: 0.7832 - val_loss: 0.9050 - val_acc: 0.7500\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8237 - acc: 0.7853 - val_loss: 0.9196 - val_acc: 0.7560\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8361 - acc: 0.7844 - val_loss: 0.9051 - val_acc: 0.7540\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8258 - acc: 0.7876 - val_loss: 0.9070 - val_acc: 0.7500\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8473 - acc: 0.7769 - val_loss: 0.9108 - val_acc: 0.7560\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8259 - acc: 0.7880 - val_loss: 0.9350 - val_acc: 0.7380\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8282 - acc: 0.7873 - val_loss: 0.9106 - val_acc: 0.7480\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8286 - acc: 0.7868 - val_loss: 0.9156 - val_acc: 0.7550\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8237 - acc: 0.7893 - val_loss: 0.9030 - val_acc: 0.7510\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8276 - acc: 0.7832 - val_loss: 0.9108 - val_acc: 0.7520\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8257 - acc: 0.7860 - val_loss: 0.9112 - val_acc: 0.7410\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8309 - acc: 0.7869 - val_loss: 0.9521 - val_acc: 0.7380\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8280 - acc: 0.7845 - val_loss: 0.9696 - val_acc: 0.7360\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8270 - acc: 0.7884 - val_loss: 0.9105 - val_acc: 0.7500\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8310 - acc: 0.7837 - val_loss: 0.9016 - val_acc: 0.7510\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8304 - acc: 0.7835 - val_loss: 0.9424 - val_acc: 0.7310\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8340 - acc: 0.7843 - val_loss: 0.9047 - val_acc: 0.7600\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8295 - acc: 0.7855 - val_loss: 0.9315 - val_acc: 0.7510\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8238 - acc: 0.7868 - val_loss: 0.8993 - val_acc: 0.7610\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8322 - acc: 0.7883 - val_loss: 1.0494 - val_acc: 0.7010\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8368 - acc: 0.7863 - val_loss: 0.9339 - val_acc: 0.7440\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8268 - acc: 0.7843 - val_loss: 0.9112 - val_acc: 0.7440\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8301 - acc: 0.7845 - val_loss: 1.0133 - val_acc: 0.7250\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8349 - acc: 0.7837 - val_loss: 0.9157 - val_acc: 0.7590\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8260 - acc: 0.7892 - val_loss: 1.0135 - val_acc: 0.7170\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8371 - acc: 0.7832 - val_loss: 0.9658 - val_acc: 0.7410\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8198 - acc: 0.7915 - val_loss: 0.9972 - val_acc: 0.7110\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8295 - acc: 0.7849 - val_loss: 0.9167 - val_acc: 0.7570\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8244 - acc: 0.7883 - val_loss: 0.9071 - val_acc: 0.7550\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8322 - acc: 0.7840 - val_loss: 1.0243 - val_acc: 0.7120\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8316 - acc: 0.7871 - val_loss: 0.9175 - val_acc: 0.7490\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8234 - acc: 0.7871 - val_loss: 0.9184 - val_acc: 0.7500\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8248 - acc: 0.7885 - val_loss: 0.9022 - val_acc: 0.7570\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8246 - acc: 0.7877 - val_loss: 0.8974 - val_acc: 0.7620\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8214 - acc: 0.7896 - val_loss: 0.9291 - val_acc: 0.7380\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8345 - acc: 0.7827 - val_loss: 0.9368 - val_acc: 0.7520\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8276 - acc: 0.7877 - val_loss: 0.9253 - val_acc: 0.7430\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8214 - acc: 0.7884 - val_loss: 0.8981 - val_acc: 0.7600\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8252 - acc: 0.7863 - val_loss: 0.9346 - val_acc: 0.7400\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8245 - acc: 0.7881 - val_loss: 0.9171 - val_acc: 0.7440\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8246 - acc: 0.7897 - val_loss: 0.9061 - val_acc: 0.7520\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8343 - acc: 0.7821 - val_loss: 0.9690 - val_acc: 0.7200\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8253 - acc: 0.7892 - val_loss: 0.9053 - val_acc: 0.7520\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8215 - acc: 0.7896 - val_loss: 0.9166 - val_acc: 0.7480\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8178 - acc: 0.7893 - val_loss: 0.9036 - val_acc: 0.7540\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8212 - acc: 0.7889 - val_loss: 0.9278 - val_acc: 0.7530\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8390 - acc: 0.7777 - val_loss: 0.9282 - val_acc: 0.7520\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8218 - acc: 0.7877 - val_loss: 0.9002 - val_acc: 0.7550\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8194 - acc: 0.7900 - val_loss: 0.9232 - val_acc: 0.7380\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8424 - acc: 0.7804 - val_loss: 0.9142 - val_acc: 0.7470\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8270 - acc: 0.7847 - val_loss: 0.9092 - val_acc: 0.7450\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8235 - acc: 0.7848 - val_loss: 0.9002 - val_acc: 0.7520\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8232 - acc: 0.7873 - val_loss: 0.9134 - val_acc: 0.7590\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8198 - acc: 0.7899 - val_loss: 0.9051 - val_acc: 0.7550\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8214 - acc: 0.7876 - val_loss: 0.9239 - val_acc: 0.7560\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8232 - acc: 0.7845 - val_loss: 0.8987 - val_acc: 0.7570\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8175 - acc: 0.7897 - val_loss: 0.9590 - val_acc: 0.7250\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8281 - acc: 0.7848 - val_loss: 0.9164 - val_acc: 0.7490\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8256 - acc: 0.7885 - val_loss: 0.9185 - val_acc: 0.7540\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8236 - acc: 0.7872 - val_loss: 0.9032 - val_acc: 0.7490\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8232 - acc: 0.7905 - val_loss: 0.9059 - val_acc: 0.7660\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8222 - acc: 0.7864 - val_loss: 0.9083 - val_acc: 0.7450\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8269 - acc: 0.7860 - val_loss: 0.9303 - val_acc: 0.7480\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8265 - acc: 0.7843 - val_loss: 0.9143 - val_acc: 0.7510\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8279 - acc: 0.7877 - val_loss: 0.9133 - val_acc: 0.7510\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8251 - acc: 0.7884 - val_loss: 0.9198 - val_acc: 0.7530\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8256 - acc: 0.7884 - val_loss: 0.9177 - val_acc: 0.7430\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8198 - acc: 0.7872 - val_loss: 0.9229 - val_acc: 0.7530\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8291 - acc: 0.7848 - val_loss: 0.9438 - val_acc: 0.7380\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8181 - acc: 0.7901 - val_loss: 0.9076 - val_acc: 0.7510\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8245 - acc: 0.7884 - val_loss: 1.0182 - val_acc: 0.7180\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8244 - acc: 0.7875 - val_loss: 0.9022 - val_acc: 0.7580\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8207 - acc: 0.7889 - val_loss: 0.9024 - val_acc: 0.7580\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8229 - acc: 0.7868 - val_loss: 0.8963 - val_acc: 0.7580\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8174 - acc: 0.7872 - val_loss: 0.9193 - val_acc: 0.7420\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8125 - acc: 0.7921 - val_loss: 0.9108 - val_acc: 0.7500\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8204 - acc: 0.7863 - val_loss: 0.9213 - val_acc: 0.7430\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8237 - acc: 0.7877 - val_loss: 0.9439 - val_acc: 0.7500\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8181 - acc: 0.7888 - val_loss: 0.9018 - val_acc: 0.7640\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8142 - acc: 0.7916 - val_loss: 0.9348 - val_acc: 0.7360\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8332 - acc: 0.7861 - val_loss: 1.0840 - val_acc: 0.7010\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8473 - acc: 0.7773 - val_loss: 0.9088 - val_acc: 0.7450\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8198 - acc: 0.7892 - val_loss: 0.9025 - val_acc: 0.7580\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8207 - acc: 0.7892 - val_loss: 0.9689 - val_acc: 0.7480\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8290 - acc: 0.7849 - val_loss: 0.9068 - val_acc: 0.7600\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8176 - acc: 0.7903 - val_loss: 0.8961 - val_acc: 0.7610\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8165 - acc: 0.7912 - val_loss: 0.9112 - val_acc: 0.7540\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8222 - acc: 0.7911 - val_loss: 0.9376 - val_acc: 0.7360\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8317 - acc: 0.7848 - val_loss: 0.9414 - val_acc: 0.7330\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8237 - acc: 0.7887 - val_loss: 0.8942 - val_acc: 0.7570\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8133 - acc: 0.7908 - val_loss: 0.9195 - val_acc: 0.7460\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8182 - acc: 0.7879 - val_loss: 0.9517 - val_acc: 0.7500\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8250 - acc: 0.7849 - val_loss: 0.9229 - val_acc: 0.7400\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8141 - acc: 0.7907 - val_loss: 0.9555 - val_acc: 0.7380\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8180 - acc: 0.7881 - val_loss: 0.9216 - val_acc: 0.7490\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8149 - acc: 0.7923 - val_loss: 0.9740 - val_acc: 0.7210\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8139 - acc: 0.7913 - val_loss: 0.9390 - val_acc: 0.7500\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8151 - acc: 0.7915 - val_loss: 0.9131 - val_acc: 0.7500\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8253 - acc: 0.7872 - val_loss: 1.0499 - val_acc: 0.7080\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8290 - acc: 0.7837 - val_loss: 0.9016 - val_acc: 0.7600\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8186 - acc: 0.7901 - val_loss: 0.9081 - val_acc: 0.7430\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8136 - acc: 0.7908 - val_loss: 0.8963 - val_acc: 0.7560\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8164 - acc: 0.7907 - val_loss: 0.9002 - val_acc: 0.7560\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8361 - acc: 0.7832 - val_loss: 0.9840 - val_acc: 0.7230\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8249 - acc: 0.7845 - val_loss: 0.8999 - val_acc: 0.7600\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8127 - acc: 0.7915 - val_loss: 0.9021 - val_acc: 0.7600\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8135 - acc: 0.7924 - val_loss: 0.9342 - val_acc: 0.7580\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8281 - acc: 0.7849 - val_loss: 0.9628 - val_acc: 0.7530\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8491 - acc: 0.7763 - val_loss: 0.9217 - val_acc: 0.7590\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8269 - acc: 0.7848 - val_loss: 0.9043 - val_acc: 0.7550\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8151 - acc: 0.7923 - val_loss: 0.8946 - val_acc: 0.7690\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8173 - acc: 0.7889 - val_loss: 0.9268 - val_acc: 0.7500\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8161 - acc: 0.7883 - val_loss: 0.9345 - val_acc: 0.7510\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8187 - acc: 0.7889 - val_loss: 0.8992 - val_acc: 0.7660\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8175 - acc: 0.7899 - val_loss: 0.9895 - val_acc: 0.7280\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8172 - acc: 0.7915 - val_loss: 0.9688 - val_acc: 0.7340\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8155 - acc: 0.7917 - val_loss: 0.9130 - val_acc: 0.7580\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8166 - acc: 0.7871 - val_loss: 0.9763 - val_acc: 0.7270\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8243 - acc: 0.7865 - val_loss: 0.9309 - val_acc: 0.7390\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8118 - acc: 0.7893 - val_loss: 0.9076 - val_acc: 0.7590\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 23us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8104417023658752, 0.7903999999682109]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9190531539916992, 0.7506666663487752]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9761 - acc: 0.1784 - val_loss: 1.9407 - val_acc: 0.1740\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9522 - acc: 0.1840 - val_loss: 1.9312 - val_acc: 0.1890\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9359 - acc: 0.1935 - val_loss: 1.9242 - val_acc: 0.1940\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9289 - acc: 0.1936 - val_loss: 1.9174 - val_acc: 0.2100\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9209 - acc: 0.2000 - val_loss: 1.9105 - val_acc: 0.2120\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9146 - acc: 0.1987 - val_loss: 1.9040 - val_acc: 0.2160\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9033 - acc: 0.2157 - val_loss: 1.8960 - val_acc: 0.2200\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8959 - acc: 0.2092 - val_loss: 1.8850 - val_acc: 0.2200\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8850 - acc: 0.2179 - val_loss: 1.8740 - val_acc: 0.2260\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8796 - acc: 0.2240 - val_loss: 1.8631 - val_acc: 0.2320\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8734 - acc: 0.2297 - val_loss: 1.8483 - val_acc: 0.2430\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8563 - acc: 0.2300 - val_loss: 1.8329 - val_acc: 0.2560\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8487 - acc: 0.2405 - val_loss: 1.8161 - val_acc: 0.2730\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8292 - acc: 0.2547 - val_loss: 1.7985 - val_acc: 0.2840\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8222 - acc: 0.2535 - val_loss: 1.7819 - val_acc: 0.3070\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8093 - acc: 0.2644 - val_loss: 1.7585 - val_acc: 0.3150\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7891 - acc: 0.2756 - val_loss: 1.7351 - val_acc: 0.3390\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7786 - acc: 0.2776 - val_loss: 1.7143 - val_acc: 0.3590\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7577 - acc: 0.2913 - val_loss: 1.6902 - val_acc: 0.3750\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7338 - acc: 0.3040 - val_loss: 1.6625 - val_acc: 0.3910\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7195 - acc: 0.3253 - val_loss: 1.6340 - val_acc: 0.4000\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7058 - acc: 0.3192 - val_loss: 1.6102 - val_acc: 0.4270\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6835 - acc: 0.3292 - val_loss: 1.5836 - val_acc: 0.4420\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6722 - acc: 0.3377 - val_loss: 1.5532 - val_acc: 0.4520\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6370 - acc: 0.3564 - val_loss: 1.5271 - val_acc: 0.4620\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6290 - acc: 0.3635 - val_loss: 1.4987 - val_acc: 0.4760\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6045 - acc: 0.3725 - val_loss: 1.4691 - val_acc: 0.4940\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5916 - acc: 0.3764 - val_loss: 1.4453 - val_acc: 0.5100\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5740 - acc: 0.3827 - val_loss: 1.4169 - val_acc: 0.5190\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5498 - acc: 0.4011 - val_loss: 1.3923 - val_acc: 0.5430\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5265 - acc: 0.4085 - val_loss: 1.3670 - val_acc: 0.5620\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5192 - acc: 0.4124 - val_loss: 1.3432 - val_acc: 0.5710\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4947 - acc: 0.4192 - val_loss: 1.3194 - val_acc: 0.5850\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4707 - acc: 0.4299 - val_loss: 1.2931 - val_acc: 0.5960\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4482 - acc: 0.4375 - val_loss: 1.2685 - val_acc: 0.6070\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4388 - acc: 0.4475 - val_loss: 1.2451 - val_acc: 0.6160\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4274 - acc: 0.4437 - val_loss: 1.2269 - val_acc: 0.6260\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4031 - acc: 0.4579 - val_loss: 1.2042 - val_acc: 0.6240\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3862 - acc: 0.4783 - val_loss: 1.1878 - val_acc: 0.6320\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3777 - acc: 0.4791 - val_loss: 1.1674 - val_acc: 0.6340\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3661 - acc: 0.4855 - val_loss: 1.1483 - val_acc: 0.6390\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3535 - acc: 0.4856 - val_loss: 1.1281 - val_acc: 0.6490\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3283 - acc: 0.4900 - val_loss: 1.1101 - val_acc: 0.6530\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3269 - acc: 0.4923 - val_loss: 1.0965 - val_acc: 0.6560\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3124 - acc: 0.4991 - val_loss: 1.0795 - val_acc: 0.6580\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2973 - acc: 0.5141 - val_loss: 1.0651 - val_acc: 0.6700\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2852 - acc: 0.5179 - val_loss: 1.0499 - val_acc: 0.6720\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2837 - acc: 0.5107 - val_loss: 1.0375 - val_acc: 0.6760\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2565 - acc: 0.5185 - val_loss: 1.0199 - val_acc: 0.6810\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2439 - acc: 0.5319 - val_loss: 1.0078 - val_acc: 0.6780\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2493 - acc: 0.5260 - val_loss: 0.9973 - val_acc: 0.6830\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2343 - acc: 0.5339 - val_loss: 0.9871 - val_acc: 0.6870\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2171 - acc: 0.5469 - val_loss: 0.9714 - val_acc: 0.6860\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2168 - acc: 0.5384 - val_loss: 0.9613 - val_acc: 0.6960\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1905 - acc: 0.5524 - val_loss: 0.9506 - val_acc: 0.6960\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1984 - acc: 0.5452 - val_loss: 0.9430 - val_acc: 0.6960\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1967 - acc: 0.5477 - val_loss: 0.9341 - val_acc: 0.6950\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1701 - acc: 0.5587 - val_loss: 0.9226 - val_acc: 0.7020\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1612 - acc: 0.5585 - val_loss: 0.9113 - val_acc: 0.7070\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1551 - acc: 0.5635 - val_loss: 0.9054 - val_acc: 0.7100\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1523 - acc: 0.5631 - val_loss: 0.8995 - val_acc: 0.7060\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1366 - acc: 0.5771 - val_loss: 0.8885 - val_acc: 0.7080\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1506 - acc: 0.5620 - val_loss: 0.8790 - val_acc: 0.7150\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1197 - acc: 0.5799 - val_loss: 0.8686 - val_acc: 0.7160\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1104 - acc: 0.5763 - val_loss: 0.8578 - val_acc: 0.7200\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1159 - acc: 0.5800 - val_loss: 0.8560 - val_acc: 0.7160\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1036 - acc: 0.5879 - val_loss: 0.8483 - val_acc: 0.7170\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1079 - acc: 0.5835 - val_loss: 0.8448 - val_acc: 0.7210\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0955 - acc: 0.5863 - val_loss: 0.8371 - val_acc: 0.7270\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0830 - acc: 0.5963 - val_loss: 0.8296 - val_acc: 0.7260\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0820 - acc: 0.5929 - val_loss: 0.8255 - val_acc: 0.7270\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0741 - acc: 0.5889 - val_loss: 0.8173 - val_acc: 0.7320\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0619 - acc: 0.5964 - val_loss: 0.8133 - val_acc: 0.7340\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0582 - acc: 0.5985 - val_loss: 0.8056 - val_acc: 0.7330\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0492 - acc: 0.6075 - val_loss: 0.7980 - val_acc: 0.7450\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0526 - acc: 0.6076 - val_loss: 0.7919 - val_acc: 0.7440\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0467 - acc: 0.6091 - val_loss: 0.7914 - val_acc: 0.7410\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0523 - acc: 0.6144 - val_loss: 0.7861 - val_acc: 0.7470\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0317 - acc: 0.6169 - val_loss: 0.7830 - val_acc: 0.7490\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0256 - acc: 0.6107 - val_loss: 0.7764 - val_acc: 0.7470\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0296 - acc: 0.6096 - val_loss: 0.7714 - val_acc: 0.7480\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0240 - acc: 0.6183 - val_loss: 0.7671 - val_acc: 0.7510\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0190 - acc: 0.6173 - val_loss: 0.7656 - val_acc: 0.7490\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9995 - acc: 0.6217 - val_loss: 0.7599 - val_acc: 0.7540\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9958 - acc: 0.6272 - val_loss: 0.7572 - val_acc: 0.7540\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9935 - acc: 0.6296 - val_loss: 0.7524 - val_acc: 0.7540\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9863 - acc: 0.6252 - val_loss: 0.7471 - val_acc: 0.7550\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0024 - acc: 0.6237 - val_loss: 0.7444 - val_acc: 0.7550\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9717 - acc: 0.6345 - val_loss: 0.7380 - val_acc: 0.7630\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9838 - acc: 0.6321 - val_loss: 0.7433 - val_acc: 0.7570\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9649 - acc: 0.6404 - val_loss: 0.7318 - val_acc: 0.7560\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9867 - acc: 0.6327 - val_loss: 0.7336 - val_acc: 0.7540\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9566 - acc: 0.6423 - val_loss: 0.7259 - val_acc: 0.7590\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9648 - acc: 0.6353 - val_loss: 0.7219 - val_acc: 0.7560\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9677 - acc: 0.6381 - val_loss: 0.7192 - val_acc: 0.7590\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9495 - acc: 0.6460 - val_loss: 0.7160 - val_acc: 0.7650\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9482 - acc: 0.6497 - val_loss: 0.7151 - val_acc: 0.7590\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9470 - acc: 0.6456 - val_loss: 0.7124 - val_acc: 0.7660\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9517 - acc: 0.6413 - val_loss: 0.7099 - val_acc: 0.7670\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9570 - acc: 0.6453 - val_loss: 0.7098 - val_acc: 0.7640\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9368 - acc: 0.6480 - val_loss: 0.7050 - val_acc: 0.7670\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9426 - acc: 0.6428 - val_loss: 0.7046 - val_acc: 0.7670\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9374 - acc: 0.6388 - val_loss: 0.6974 - val_acc: 0.7670\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9230 - acc: 0.6519 - val_loss: 0.6947 - val_acc: 0.7640\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9273 - acc: 0.6516 - val_loss: 0.6906 - val_acc: 0.7680\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9341 - acc: 0.6527 - val_loss: 0.6909 - val_acc: 0.7710\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9246 - acc: 0.6543 - val_loss: 0.6907 - val_acc: 0.7630\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9158 - acc: 0.6560 - val_loss: 0.6897 - val_acc: 0.7620\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9096 - acc: 0.6605 - val_loss: 0.6892 - val_acc: 0.7610\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9109 - acc: 0.6591 - val_loss: 0.6809 - val_acc: 0.7640\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8973 - acc: 0.6680 - val_loss: 0.6809 - val_acc: 0.7660\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9134 - acc: 0.6589 - val_loss: 0.6764 - val_acc: 0.7660\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9051 - acc: 0.6609 - val_loss: 0.6738 - val_acc: 0.7660\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8967 - acc: 0.6608 - val_loss: 0.6719 - val_acc: 0.7670\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8942 - acc: 0.6643 - val_loss: 0.6741 - val_acc: 0.7660\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8786 - acc: 0.6664 - val_loss: 0.6721 - val_acc: 0.7610\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8981 - acc: 0.6656 - val_loss: 0.6685 - val_acc: 0.7650\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8957 - acc: 0.6644 - val_loss: 0.6682 - val_acc: 0.7670\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8824 - acc: 0.6697 - val_loss: 0.6681 - val_acc: 0.7650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8797 - acc: 0.6697 - val_loss: 0.6623 - val_acc: 0.7690\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8731 - acc: 0.6777 - val_loss: 0.6606 - val_acc: 0.7670\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8735 - acc: 0.6675 - val_loss: 0.6595 - val_acc: 0.7730\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8627 - acc: 0.6691 - val_loss: 0.6563 - val_acc: 0.7740\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8693 - acc: 0.6732 - val_loss: 0.6527 - val_acc: 0.7740\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8702 - acc: 0.6749 - val_loss: 0.6560 - val_acc: 0.7750\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8629 - acc: 0.6740 - val_loss: 0.6514 - val_acc: 0.7710\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8699 - acc: 0.6760 - val_loss: 0.6551 - val_acc: 0.7680\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8523 - acc: 0.6777 - val_loss: 0.6521 - val_acc: 0.7650\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8540 - acc: 0.6751 - val_loss: 0.6512 - val_acc: 0.7660\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8680 - acc: 0.6741 - val_loss: 0.6498 - val_acc: 0.7680\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8504 - acc: 0.6823 - val_loss: 0.6458 - val_acc: 0.7690\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8502 - acc: 0.6776 - val_loss: 0.6465 - val_acc: 0.7720\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8537 - acc: 0.6828 - val_loss: 0.6441 - val_acc: 0.7700\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8415 - acc: 0.6840 - val_loss: 0.6437 - val_acc: 0.7680\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8335 - acc: 0.6855 - val_loss: 0.6420 - val_acc: 0.7700\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8354 - acc: 0.6879 - val_loss: 0.6416 - val_acc: 0.7710\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8379 - acc: 0.6868 - val_loss: 0.6366 - val_acc: 0.7690\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8472 - acc: 0.6820 - val_loss: 0.6340 - val_acc: 0.7690\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8391 - acc: 0.6827 - val_loss: 0.6331 - val_acc: 0.7690\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8428 - acc: 0.6851 - val_loss: 0.6354 - val_acc: 0.7750\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8156 - acc: 0.6876 - val_loss: 0.6303 - val_acc: 0.7730\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8189 - acc: 0.6876 - val_loss: 0.6319 - val_acc: 0.7710\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8337 - acc: 0.6825 - val_loss: 0.6307 - val_acc: 0.7720\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8320 - acc: 0.6781 - val_loss: 0.6272 - val_acc: 0.7690\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8242 - acc: 0.6880 - val_loss: 0.6262 - val_acc: 0.7740\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8303 - acc: 0.6895 - val_loss: 0.6281 - val_acc: 0.7720\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8170 - acc: 0.6901 - val_loss: 0.6279 - val_acc: 0.7700\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8279 - acc: 0.6919 - val_loss: 0.6241 - val_acc: 0.7730\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8172 - acc: 0.6928 - val_loss: 0.6234 - val_acc: 0.7750\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8075 - acc: 0.7005 - val_loss: 0.6218 - val_acc: 0.7740\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8194 - acc: 0.6936 - val_loss: 0.6261 - val_acc: 0.7730\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8193 - acc: 0.6869 - val_loss: 0.6224 - val_acc: 0.7770\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8161 - acc: 0.6928 - val_loss: 0.6209 - val_acc: 0.7780\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7914 - acc: 0.7035 - val_loss: 0.6201 - val_acc: 0.7830\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8033 - acc: 0.6927 - val_loss: 0.6190 - val_acc: 0.7750\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8005 - acc: 0.6933 - val_loss: 0.6161 - val_acc: 0.7750\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7990 - acc: 0.6943 - val_loss: 0.6158 - val_acc: 0.7730\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7958 - acc: 0.6980 - val_loss: 0.6126 - val_acc: 0.7760\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7843 - acc: 0.6996 - val_loss: 0.6126 - val_acc: 0.7740\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8005 - acc: 0.7000 - val_loss: 0.6162 - val_acc: 0.7800\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7876 - acc: 0.7012 - val_loss: 0.6092 - val_acc: 0.7770\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7896 - acc: 0.6980 - val_loss: 0.6143 - val_acc: 0.7770\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7928 - acc: 0.7043 - val_loss: 0.6102 - val_acc: 0.7780\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7926 - acc: 0.6941 - val_loss: 0.6109 - val_acc: 0.7760\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7869 - acc: 0.7024 - val_loss: 0.6069 - val_acc: 0.7770\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7714 - acc: 0.7095 - val_loss: 0.6111 - val_acc: 0.7790\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7833 - acc: 0.7011 - val_loss: 0.6086 - val_acc: 0.7800\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7819 - acc: 0.7061 - val_loss: 0.6078 - val_acc: 0.7800\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7838 - acc: 0.7048 - val_loss: 0.6071 - val_acc: 0.7770\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7766 - acc: 0.7035 - val_loss: 0.6075 - val_acc: 0.7740\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7865 - acc: 0.7000 - val_loss: 0.6054 - val_acc: 0.7760\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7653 - acc: 0.7107 - val_loss: 0.6028 - val_acc: 0.7770\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7853 - acc: 0.7016 - val_loss: 0.6041 - val_acc: 0.7730\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7649 - acc: 0.7113 - val_loss: 0.6033 - val_acc: 0.7790\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7669 - acc: 0.7024 - val_loss: 0.6034 - val_acc: 0.7770\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7666 - acc: 0.7103 - val_loss: 0.6012 - val_acc: 0.7810\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7614 - acc: 0.7107 - val_loss: 0.6003 - val_acc: 0.7770\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7747 - acc: 0.7065 - val_loss: 0.6028 - val_acc: 0.7790\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7531 - acc: 0.7083 - val_loss: 0.5989 - val_acc: 0.7800\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7620 - acc: 0.7104 - val_loss: 0.5980 - val_acc: 0.7790\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7628 - acc: 0.7085 - val_loss: 0.5999 - val_acc: 0.7760\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7774 - acc: 0.7032 - val_loss: 0.5988 - val_acc: 0.7750\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7491 - acc: 0.7156 - val_loss: 0.5986 - val_acc: 0.7750\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7565 - acc: 0.7100 - val_loss: 0.6005 - val_acc: 0.7740\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7489 - acc: 0.7176 - val_loss: 0.5982 - val_acc: 0.7760\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7538 - acc: 0.7155 - val_loss: 0.5977 - val_acc: 0.7790\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7580 - acc: 0.7113 - val_loss: 0.5941 - val_acc: 0.7770\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7536 - acc: 0.7159 - val_loss: 0.5951 - val_acc: 0.7800\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7527 - acc: 0.7164 - val_loss: 0.5933 - val_acc: 0.7830\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7369 - acc: 0.7217 - val_loss: 0.5949 - val_acc: 0.7830\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7437 - acc: 0.7176 - val_loss: 0.5911 - val_acc: 0.7850\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7587 - acc: 0.7084 - val_loss: 0.5944 - val_acc: 0.7780\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7448 - acc: 0.7176 - val_loss: 0.5939 - val_acc: 0.7860\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7419 - acc: 0.7155 - val_loss: 0.5899 - val_acc: 0.7860\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7264 - acc: 0.7215 - val_loss: 0.5931 - val_acc: 0.7820\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7380 - acc: 0.7169 - val_loss: 0.5933 - val_acc: 0.7790\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7389 - acc: 0.7203 - val_loss: 0.5904 - val_acc: 0.7790\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7280 - acc: 0.7183 - val_loss: 0.5883 - val_acc: 0.7810\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7376 - acc: 0.7196 - val_loss: 0.5881 - val_acc: 0.7840\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7432 - acc: 0.7121 - val_loss: 0.5886 - val_acc: 0.7820\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step\n",
      "1500/1500 [==============================] - 0s 28us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4612004637877146, 0.8265333333015442]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6273041297594706, 0.7606666663487752]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 1.9290 - acc: 0.1880 - val_loss: 1.8976 - val_acc: 0.1937\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 1.8562 - acc: 0.2219 - val_loss: 1.8195 - val_acc: 0.2503\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.7485 - acc: 0.3133 - val_loss: 1.6876 - val_acc: 0.3723\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.5818 - acc: 0.4647 - val_loss: 1.5032 - val_acc: 0.5090\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.3775 - acc: 0.5853 - val_loss: 1.2973 - val_acc: 0.6080\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.1783 - acc: 0.6549 - val_loss: 1.1208 - val_acc: 0.6473\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.0213 - acc: 0.6924 - val_loss: 0.9897 - val_acc: 0.6727\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.9081 - acc: 0.7130 - val_loss: 0.8979 - val_acc: 0.6937\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.8287 - acc: 0.7254 - val_loss: 0.8329 - val_acc: 0.7057\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.7723 - acc: 0.7355 - val_loss: 0.7847 - val_acc: 0.7163\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.7308 - acc: 0.7447 - val_loss: 0.7504 - val_acc: 0.7270\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6994 - acc: 0.7532 - val_loss: 0.7230 - val_acc: 0.7363\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6747 - acc: 0.7602 - val_loss: 0.7033 - val_acc: 0.7440\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6544 - acc: 0.7645 - val_loss: 0.6864 - val_acc: 0.7490\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6374 - acc: 0.7701 - val_loss: 0.6738 - val_acc: 0.7523\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6229 - acc: 0.7738 - val_loss: 0.6600 - val_acc: 0.7560\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6098 - acc: 0.7789 - val_loss: 0.6490 - val_acc: 0.7607\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.5984 - acc: 0.7832 - val_loss: 0.6404 - val_acc: 0.7637\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5880 - acc: 0.7865 - val_loss: 0.6318 - val_acc: 0.7680\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5785 - acc: 0.7888 - val_loss: 0.6263 - val_acc: 0.7707\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5695 - acc: 0.7926 - val_loss: 0.6179 - val_acc: 0.7750\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5614 - acc: 0.7949 - val_loss: 0.6123 - val_acc: 0.7747\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5538 - acc: 0.7976 - val_loss: 0.6072 - val_acc: 0.7807\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5461 - acc: 0.8011 - val_loss: 0.6032 - val_acc: 0.7813\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5395 - acc: 0.8049 - val_loss: 0.5972 - val_acc: 0.7830\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5327 - acc: 0.8076 - val_loss: 0.5927 - val_acc: 0.7857\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5264 - acc: 0.8097 - val_loss: 0.5932 - val_acc: 0.7823\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5206 - acc: 0.8120 - val_loss: 0.5856 - val_acc: 0.7863\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5147 - acc: 0.8148 - val_loss: 0.5826 - val_acc: 0.7883\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.5091 - acc: 0.8171 - val_loss: 0.5783 - val_acc: 0.7903\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5043 - acc: 0.8175 - val_loss: 0.5760 - val_acc: 0.7893\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4986 - acc: 0.8199 - val_loss: 0.5806 - val_acc: 0.7887\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4942 - acc: 0.8227 - val_loss: 0.5702 - val_acc: 0.7927\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4896 - acc: 0.8239 - val_loss: 0.5673 - val_acc: 0.7990\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4851 - acc: 0.8255 - val_loss: 0.5646 - val_acc: 0.7960\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4806 - acc: 0.8271 - val_loss: 0.5629 - val_acc: 0.7960\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4760 - acc: 0.8299 - val_loss: 0.5613 - val_acc: 0.7967\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4720 - acc: 0.8315 - val_loss: 0.5589 - val_acc: 0.7987\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4678 - acc: 0.8329 - val_loss: 0.5575 - val_acc: 0.8007\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4640 - acc: 0.8340 - val_loss: 0.5586 - val_acc: 0.8007\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4601 - acc: 0.8353 - val_loss: 0.5575 - val_acc: 0.7977\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4563 - acc: 0.8378 - val_loss: 0.5543 - val_acc: 0.7993\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4528 - acc: 0.8394 - val_loss: 0.5529 - val_acc: 0.8000\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4496 - acc: 0.8402 - val_loss: 0.5511 - val_acc: 0.8023\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4460 - acc: 0.8417 - val_loss: 0.5508 - val_acc: 0.8037\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4424 - acc: 0.8429 - val_loss: 0.5493 - val_acc: 0.8020\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4390 - acc: 0.8439 - val_loss: 0.5488 - val_acc: 0.8030\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4355 - acc: 0.8458 - val_loss: 0.5456 - val_acc: 0.8043\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4325 - acc: 0.8462 - val_loss: 0.5454 - val_acc: 0.8043\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4298 - acc: 0.8488 - val_loss: 0.5509 - val_acc: 0.8040\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8489 - val_loss: 0.5460 - val_acc: 0.8063\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4233 - acc: 0.8509 - val_loss: 0.5457 - val_acc: 0.8043\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.4204 - acc: 0.8519 - val_loss: 0.5433 - val_acc: 0.8067\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4178 - acc: 0.8531 - val_loss: 0.5430 - val_acc: 0.8077\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4150 - acc: 0.8544 - val_loss: 0.5430 - val_acc: 0.8050\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4122 - acc: 0.8551 - val_loss: 0.5439 - val_acc: 0.8083\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4096 - acc: 0.8564 - val_loss: 0.5444 - val_acc: 0.8093\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4072 - acc: 0.8576 - val_loss: 0.5424 - val_acc: 0.8083\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4045 - acc: 0.8583 - val_loss: 0.5464 - val_acc: 0.8040\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4021 - acc: 0.8587 - val_loss: 0.5439 - val_acc: 0.8047\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3996 - acc: 0.8603 - val_loss: 0.5456 - val_acc: 0.8097\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3970 - acc: 0.8611 - val_loss: 0.5440 - val_acc: 0.8077\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3945 - acc: 0.8632 - val_loss: 0.5411 - val_acc: 0.8057\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3923 - acc: 0.8638 - val_loss: 0.5428 - val_acc: 0.8083\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3899 - acc: 0.8643 - val_loss: 0.5404 - val_acc: 0.8077\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3874 - acc: 0.8652 - val_loss: 0.5422 - val_acc: 0.8103\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3852 - acc: 0.8663 - val_loss: 0.5426 - val_acc: 0.8087\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3830 - acc: 0.8667 - val_loss: 0.5438 - val_acc: 0.8093\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3809 - acc: 0.8688 - val_loss: 0.5438 - val_acc: 0.8107\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3789 - acc: 0.8685 - val_loss: 0.5411 - val_acc: 0.8097\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3763 - acc: 0.8699 - val_loss: 0.5430 - val_acc: 0.8093\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3745 - acc: 0.8710 - val_loss: 0.5438 - val_acc: 0.8087\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3724 - acc: 0.8710 - val_loss: 0.5437 - val_acc: 0.8103\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3705 - acc: 0.8717 - val_loss: 0.5517 - val_acc: 0.8073\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3686 - acc: 0.8732 - val_loss: 0.5445 - val_acc: 0.8130\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3665 - acc: 0.8741 - val_loss: 0.5430 - val_acc: 0.8137\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3644 - acc: 0.8739 - val_loss: 0.5482 - val_acc: 0.8120\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3624 - acc: 0.8748 - val_loss: 0.5452 - val_acc: 0.8107\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3601 - acc: 0.8767 - val_loss: 0.5469 - val_acc: 0.8110\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3583 - acc: 0.8772 - val_loss: 0.5467 - val_acc: 0.8110\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3568 - acc: 0.8773 - val_loss: 0.5513 - val_acc: 0.8100\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3547 - acc: 0.8777 - val_loss: 0.5475 - val_acc: 0.8117\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3528 - acc: 0.8793 - val_loss: 0.5498 - val_acc: 0.8120\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3511 - acc: 0.8797 - val_loss: 0.5528 - val_acc: 0.8110\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3497 - acc: 0.8799 - val_loss: 0.5515 - val_acc: 0.8083\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3475 - acc: 0.8804 - val_loss: 0.5512 - val_acc: 0.8100\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3459 - acc: 0.8824 - val_loss: 0.5533 - val_acc: 0.8077\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3437 - acc: 0.8816 - val_loss: 0.5544 - val_acc: 0.8093\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3421 - acc: 0.8830 - val_loss: 0.5510 - val_acc: 0.8093\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3407 - acc: 0.8839 - val_loss: 0.5553 - val_acc: 0.8083\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3387 - acc: 0.8859 - val_loss: 0.5565 - val_acc: 0.8123\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3373 - acc: 0.8844 - val_loss: 0.5608 - val_acc: 0.8080\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3355 - acc: 0.8848 - val_loss: 0.5553 - val_acc: 0.8070\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3340 - acc: 0.8866 - val_loss: 0.5547 - val_acc: 0.8103\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3325 - acc: 0.8867 - val_loss: 0.5559 - val_acc: 0.8097\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3307 - acc: 0.8875 - val_loss: 0.5589 - val_acc: 0.8070\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3289 - acc: 0.8882 - val_loss: 0.5576 - val_acc: 0.8080\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3277 - acc: 0.8886 - val_loss: 0.5590 - val_acc: 0.8100\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3260 - acc: 0.8881 - val_loss: 0.5613 - val_acc: 0.8073\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3244 - acc: 0.8896 - val_loss: 0.5637 - val_acc: 0.8067\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3229 - acc: 0.8909 - val_loss: 0.5671 - val_acc: 0.8070\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3212 - acc: 0.8900 - val_loss: 0.5672 - val_acc: 0.8087\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3195 - acc: 0.8925 - val_loss: 0.5712 - val_acc: 0.8063\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3183 - acc: 0.8919 - val_loss: 0.5670 - val_acc: 0.8063\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3164 - acc: 0.8920 - val_loss: 0.5655 - val_acc: 0.8070\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3152 - acc: 0.8938 - val_loss: 0.5707 - val_acc: 0.8067\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3134 - acc: 0.8938 - val_loss: 0.5726 - val_acc: 0.8063\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3119 - acc: 0.8948 - val_loss: 0.5676 - val_acc: 0.8047\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3106 - acc: 0.8944 - val_loss: 0.5720 - val_acc: 0.8073\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.3092 - acc: 0.8956 - val_loss: 0.5700 - val_acc: 0.8070\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3079 - acc: 0.8955 - val_loss: 0.5714 - val_acc: 0.8027\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.8972 - val_loss: 0.5720 - val_acc: 0.8047\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3049 - acc: 0.8970 - val_loss: 0.5754 - val_acc: 0.8060\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.8984 - val_loss: 0.5810 - val_acc: 0.8063\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3022 - acc: 0.8984 - val_loss: 0.5778 - val_acc: 0.8060\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.3002 - acc: 0.8997 - val_loss: 0.5835 - val_acc: 0.8043\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2994 - acc: 0.8990 - val_loss: 0.5783 - val_acc: 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2977 - acc: 0.8991 - val_loss: 0.5822 - val_acc: 0.8053\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2960 - acc: 0.9013 - val_loss: 0.5809 - val_acc: 0.8057\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.2949 - acc: 0.9016 - val_loss: 0.5831 - val_acc: 0.8067\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 24us/step\n",
      "4000/4000 [==============================] - 0s 28us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28893913805123533, 0.9047272727272727]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5692696625590324, 0.812]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
